% !TEX root = ../main.tex
% The abstract.
% Included by MAIN.TEX
\clearemptydoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Abstract}

\vspace*{2cm}
\begin{center}
{\Large \textbf{Abstract}}
\end{center}
\vspace{1cm}

\chapter*{Abstract}
\label{chapter:Abstract}

Reinforcement learning (RL) has shown remarkable progress in recent years, but still faces challenges in solving tasks with high-dimensional 
continuous state and action spaces, and sparse rewards. This thesis proposes a new RL paradigm called Active Critic (AC), which uses search at 
inference time and leverages expert demonstrations to overcome these challenges. AC is a general-purpose algorithm applicable to both deterministic 
Markov decision processes (MDPs) and deterministic partially observable MDPs (POMDPs), and is the first algorithm to use inference-time search on continuous 
action spaces without a given model of the environment.\\

In this thesis, we investigate the effectiveness of AC in solving tasks with sparse rewards and limited observations. We test AC on 
a setup where a single observation per trajectory is provided along with expert demonstrations, as well as on an MDP setting with observations 
after each step. We compare AC's performance to state-of-the-art RL algorithms, TQC and PPO, as well as to GAIL, a state-of-the-art imitation 
learning algorithm. \\

Our experimental results show that AC outperforms the selected baselines in both the single observation POMDPs and MDPs, achieving superior 
performance while maintaining high sample efficiency. We also propose several avenues for future research, such as incorporating stochastic 
observations, modeling uncertainty estimates, and incorporating curiosity-driven exploration.\\

Overall, the results demonstrate the efficacy of the AC paradigm in leveraging expert knowledge and search to achieve swift learning and 
increased stability in challenging RL tasks.