@misc{latex,
		title={the not so short introduction to LaTeX2U+03B5: or Latex2U+03B5 in
				 157 minutes},
    url={https://tobi.oetiker.ch/lshort/lshort.pdf},
		author={tobias oetiker}
}

@book{bishop,
	title={Pattern Recognition and Machine Learning},
	author={Chritopher M. Bishop},
	year={2006},
	publisher={Springer Science+Business Media, LLC}
	}

@book{Goodfellow,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@book{sutton2015reinforcement,
	title={Reinforcement Learning: An Introduction},
	author={Sutton, Richard S and Barto, Andrew G},
	year={2015},
	edition={Second edition, in progress},
	publisher={The MIT Press},
	address={Cambridge, Massachusetts, London, England},
}

@online{optimality,
    author = {Vaibhav Kumar},
    title = {Mathematical Analysis of Reinforcement Learning — Bellman Optimality Equation},
    year = {2020},
    url = {https://towardsdatascience.com/mathematical-analysis-of-reinforcement-learning-bellman-equation-ac9f0954e19f},
    urldate = {2023-02-20},
}

@article{Watkins1992,
  author  = {Watkins, Christopher J. C. H. and Dayan, Peter},
  title   = {Q-learning},
  journal = {Machine Learning},
  year    = {1992},
  volume  = {8},
  number  = {3},
  pages   = {279--292},
  doi     = {10.1007/BF00992698},
  url     = {https://doi.org/10.1007/BF00992698},
  issn    = {1573-0565},
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
}

@online{haarnoja2018soft,
	title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
	author={Tuomas Haarnoja, et al.},
	url={arXiv:1801.01290},
	year={2018},
	urldate = {2023-02-20},
}

@misc{PPO,
  doi = {10.48550/ARXIV.1707.06347},
  
  url = {https://arxiv.org/abs/1707.06347},
  
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Proximal Policy Optimization Algorithms},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{TRPO,
  doi = {10.48550/ARXIV.1502.05477},
  
  url = {https://arxiv.org/abs/1502.05477},
  
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Trust Region Policy Optimization},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{thrun1993issues,
  title={Issues in Using Function Approximation for Reinforcement Learning},
  author={Sebastian Thrun and Anton Schwartz},
  booktitle={Proceedings of the Fourth Connectionist Models Summer School},
  year={1993},
  organization={Lawrence Erlbaum Publisher},
  address={Hillsdale, NJ}
}

@misc{TQC_Paper,
  doi = {10.48550/ARXIV.2005.04269},
  
  url = {https://arxiv.org/abs/2005.04269},
  
  author = {Kuznetsov, Arsenii and Shvechikov, Pavel and Grishin, Alexander and Vetrov, Dmitry},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{IL,
url = {http://dx.doi.org/10.1561/2300000053},
year = {2018},
volume = {7},
journal = {Foundations and Trends® in Robotics},
title = {An Algorithmic Perspective on Imitation Learning},
doi = {10.1561/2300000053},
issn = {1935-8253},
number = {1-2},
pages = {1-179},
author = {Takayuki Osa and Joni Pajarinen and Gerhard Neumann and J. Andrew Bagnell and Pieter Abbeel and Jan Peters}
}

@INPROCEEDINGS{InfCausalEnt,

  author={Bloem, Michael and Bambos, Nicholas},

  booktitle={53rd IEEE Conference on Decision and Control}, 

  title={Infinite time horizon maximum causal entropy inverse reinforcement learning}, 

  year={2014},

  volume={},

  number={},

  pages={4911-4916},

  doi={10.1109/CDC.2014.7040156}}

@inproceedings{NEURIPS2020_b5c01503,
 author = {Xu, Tian and Li, Ziniu and Yu, Yang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {15737--15749},
 publisher = {Curran Associates, Inc.},
 title = {Error Bounds of Imitating Policies and Environments},
 url = {https://proceedings.neurips.cc/paper/2020/file/b5c01503041b70d41d80e3dbe31bbd8c-Paper.pdf},
 volume = {33},
 year = {2020}
}
$Theorem 1 shows quadratic error for imitation, 

$discounted causal entropy
$https://aviationsystems.arc.nasa.gov/publications/2015/IEEE-CDC2014_Bloem.pdf



§Overview over reinforcement learning including from imiation: https://arxiv.org/pdf/2205.00824.pdf
§Related Work: 
$Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards: https://arxiv.org/abs/1707.08817