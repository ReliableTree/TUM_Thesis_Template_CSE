@article{Manna2022,
  author = {Manna, Sukriti and Loeffler, Troy D. and Batra, Rohit and Banik, Suvo and Chan, Henry and Varughese, Bilvin and Sasikumar, Kiran and Sternberg, Michael and Peterka, Tom and Cherukara, Mathew J. and Gray, Stephen K. and Sumpter, Bobby G. and Sankaranarayanan, Subramanian K. R. S.},
  title = {Learning in continuous action space for developing high dimensional potential energy models},
  journal = {Nature Communications},
  year = {2022},
  volume = {13},
  issue = {1},
  pages = {368},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-27849-6},
  url = {https://doi.org/10.1038/s41467-021-27849-6}
}

@misc{gawlikowski2022survey,
      title={A Survey of Uncertainty in Deep Neural Networks}, 
      author={Jakob Gawlikowski and Cedrique Rovile Njieutcheu Tassi and Mohsin Ali and Jongseok Lee and Matthias Humt and Jianxiang Feng and Anna Kruspe and Rudolph Triebel and Peter Jung and Ribana Roscher and Muhammad Shahzad and Wen Yang and Richard Bamler and Xiao Xiang Zhu},
      year={2022},
      eprint={2107.03342},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{liu2022simple,
      title={A Simple Approach to Improve Single-Model Deep Uncertainty via Distance-Awareness}, 
      author={Jeremiah Zhe Liu and Shreyas Padhy and Jie Ren and Zi Lin and Yeming Wen and Ghassen Jerfel and Zack Nado and Jasper Snoek and Dustin Tran and Balaji Lakshminarayanan},
      year={2022},
      eprint={2205.00403},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{pathak2017curiositydriven,
      title={Curiosity-driven Exploration by Self-supervised Prediction}, 
      author={Deepak Pathak and Pulkit Agrawal and Alexei A. Efros and Trevor Darrell},
      year={2017},
      eprint={1705.05363},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{midjourney,
  title = {Midjourney},
  url = {https://www.midjourney.com/home/?callbackUrl=%2Fapp%2F}
}

@misc{cite:ChatGPT,
  title = {ChatGPT},
  url = {https://openai.com/blog/chatgpt}
  }



@book{bishop,
	title={Pattern Recognition and Machine Learning},
	author={Chritopher M. Bishop},
	year={2006},
	publisher={Springer Science+Business Media, LLC}
	}

@book{Goodfellow,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@book{sutton2015reinforcement,
	title={Reinforcement Learning: An Introduction},
	author={Sutton, Richard S and Barto, Andrew G},
	year={2015},
	edition={Second edition, in progress},
	publisher={The MIT Press},
	address={Cambridge, Massachusetts, London, England},
}

@online{optimality,
    author = {Vaibhav Kumar},
    title = {Mathematical Analysis of Reinforcement Learning — Bellman Optimality Equation},
    year = {2020},
    url = {https://towardsdatascience.com/mathematical-analysis-of-reinforcement-learning-bellman-equation-ac9f0954e19f},
    urldate = {2023-02-20},
}

@article{Watkins1992,
  author  = {Watkins, Christopher J. C. H. and Dayan, Peter},
  title   = {Q-learning},
  journal = {Machine Learning},
  year    = {1992},
  volume  = {8},
  number  = {3},
  pages   = {279--292},
  doi     = {10.1007/BF00992698},
  url     = {https://doi.org/10.1007/BF00992698},
  issn    = {1573-0565},
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
}



@article{A2C,
  author    = {Volodymyr Mnih and
               Adri{\`{a}} Puigdom{\`{e}}nech Badia and
               Mehdi Mirza and
               Alex Graves and
               Timothy P. Lillicrap and
               Tim Harley and
               David Silver and
               Koray Kavukcuoglu},
  title     = {Asynchronous Methods for Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1602.01783},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.01783},
  eprinttype = {arXiv},
  eprint    = {1602.01783},
  timestamp = {Mon, 13 Aug 2018 16:47:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MnihBMGLHSK16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Inbook{proof_A,
author="Holzleitner, Markus
and Gruber, Lukas
and Arjona-Medina, Jos{\'e}
and Brandstetter, Johannes
and Hochreiter, Sepp",
editor="Hameurlain, Abdelkader
and Tjoa, A. Min",
title="Convergence Proof for Actor-Critic Methods Applied to PPO and RUDDER",
bookTitle="Transactions on Large-Scale Data- and Knowledge-Centered Systems XLVIII: Special Issue In Memory of Univ. Prof. Dr. Roland Wagner",
year="2021",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="105--130",
abstract="We prove under commonly used assumptions the convergence of actor-critic reinforcement learning algorithms, which simultaneously learn a policy function, the actor, and a value function, the critic. Both functions can be deep neural networks of arbitrary complexity. Our framework allows showing convergence of the well known Proximal Policy Optimization (PPO) and of the recently introduced RUDDER. For the convergence proof we employ recently introduced techniques from the two time-scale stochastic approximation theory.",
isbn="978-3-662-63519-3",
doi="10.1007/978-3-662-63519-3_5",
url="https://doi.org/10.1007/978-3-662-63519-3_5"
}


@online{haarnoja2018soft,
	title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
	author={Tuomas Haarnoja, et al.},
	url={arXiv:1801.01290},
	year={2018},
	urldate = {2023-02-20},
}

@misc{PPO,
  doi = {10.48550/ARXIV.1707.06347},
  
  url = {https://arxiv.org/abs/1707.06347},
  
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Proximal Policy Optimization Algorithms},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{TRPO,
  doi = {10.48550/ARXIV.1502.05477},
  
  url = {https://arxiv.org/abs/1502.05477},
  
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Trust Region Policy Optimization},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{thrun1993issues,
  title={Issues in Using Function Approximation for Reinforcement Learning},
  author={Sebastian Thrun and Anton Schwartz},
  booktitle={Proceedings of the Fourth Connectionist Models Summer School},
  year={1993},
  organization={Lawrence Erlbaum Publisher},
  address={Hillsdale, NJ}
}

@misc{TQC_Paper,
  doi = {10.48550/ARXIV.2005.04269},
  
  url = {https://arxiv.org/abs/2005.04269},
  
  author = {Kuznetsov, Arsenii and Shvechikov, Pavel and Grishin, Alexander and Vetrov, Dmitry},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{RPPO,
  doi = {10.48550/ARXIV.2205.11104},
  
  url = {https://arxiv.org/abs/2205.11104},
  
  author = {Pleines, Marco and Pallasch, Matthias and Zimmer, Frank and Preuss, Mike},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Generalization, Mayhems and Limits in Recurrent Proximal Policy Optimization},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{IL,
url = {http://dx.doi.org/10.1561/2300000053},
year = {2018},
volume = {7},
journal = {Foundations and Trends® in Robotics},
title = {An Algorithmic Perspective on Imitation Learning},
doi = {10.1561/2300000053},
issn = {1935-8253},
number = {1-2},
pages = {1-179},
author = {Takayuki Osa and Joni Pajarinen and Gerhard Neumann and J. Andrew Bagnell and Pieter Abbeel and Jan Peters}
}

@INPROCEEDINGS{InfCausalEnt,

  author={Bloem, Michael and Bambos, Nicholas},

  booktitle={53rd IEEE Conference on Decision and Control}, 

  title={Infinite time horizon maximum causal entropy inverse reinforcement learning}, 

  year={2014},

  volume={},

  number={},

  pages={4911-4916},

  doi={10.1109/CDC.2014.7040156}}

@inproceedings{NEURIPS2020_b5c01503,
 author = {Xu, Tian and Li, Ziniu and Yu, Yang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {15737--15749},
 publisher = {Curran Associates, Inc.},
 title = {Error Bounds of Imitating Policies and Environments},
 url = {https://proceedings.neurips.cc/paper/2020/file/b5c01503041b70d41d80e3dbe31bbd8c-Paper.pdf},
 volume = {33},
 year = {2020}
}


@article{MUZero,
	doi = {10.1038/s41586-020-03051-4},
  
	url = {https://doi.org/10.1038%2Fs41586-020-03051-4},
  
	year = 2020,
	month = {dec},
  
	publisher = {Springer Science and Business Media {LLC}
},
  
	volume = {588},
  
	number = {7839},
  
	pages = {604--609},
  
	author = {Julian Schrittwieser and Ioannis Antonoglou and Thomas Hubert and Karen Simonyan and Laurent Sifre and Simon Schmitt and Arthur Guez and Edward Lockhart and Demis Hassabis and Thore Graepel and Timothy Lillicrap and David Silver},
  
	title = {Mastering Atari, Go, chess and shogi by planning with a learned model},
  
	journal = {Nature}
}

@misc{grimm2021proper,
      title={Proper Value Equivalence}, 
      author={Christopher Grimm and André Barreto and Gregory Farquhar and David Silver and Satinder Singh},
      year={2021},
      eprint={2106.10316},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{Lee_Jeon_Kim_Kim_2020, 
    title={Monte-Carlo Tree Search in Continuous Action Spaces with Value Gradients}, 
    volume={34}, 
    url={https://ojs.aaai.org/index.php/AAAI/article/view/5885}, 
    DOI={10.1609/aaai.v34i04.5885}, 
    number={04}, 
    journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
    author={Lee, Jongmin and Jeon, Wonseok and Kim, Geon-Hyeong and Kim, Kee-Eung}, 
    year={2020}, 
    month={Apr.}, 
    pages={4561-4568}
 }

 @misc{ho2016generative,
      title={Generative Adversarial Imitation Learning}, 
      author={Jonathan Ho and Stefano Ermon},
      year={2016},
      eprint={1606.03476},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

 @misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{stepputtis2020languageconditioned,
      title={Language-Conditioned Imitation Learning for Robot Manipulation Tasks}, 
      author={Simon Stepputtis and Joseph Campbell and Mariano Phielipp and Stefan Lee and Chitta Baral and Heni Ben Amor},
      year={2020},
      eprint={2010.12083},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@inproceedings{yu2019meta,
  title={Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning},
  author={Tianhe Yu and Deirdre Quillen and Zhanpeng He and Ryan Julian and Karol Hausman and Chelsea Finn and Sergey Levine},
  booktitle={Conference on Robot Learning (CoRL)},
  year={2019},
  eprint={1910.10897},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/1910.10897}
}

@misc{1606.01540,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}

@inproceedings{pennington-etal-2014-glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}

@article{stable-baselines3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}
@misc{lillicrap2019continuous,
      title={Continuous control with deep reinforcement learning}, 
      author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
      year={2019},
      eprint={1509.02971},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{10.5555/3104322.3104481,
author = {Ziebart, Brian D. and Bagnell, J. Andrew and Dey, Anind K.},
title = {Modeling Interaction via the Principle of Maximum Causal Entropy},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {The principle of maximum entropy provides a powerful framework for statistical models of joint, conditional, and marginal distributions. However, there are many important distributions with elements of interaction and feedback where its applicability has not been established. This work presents the principle of maximum causal entropy—an approach based on causally conditioned probabilities that can appropriately model the availability and influence of sequentially revealed side information. Using this principle, we derive models for sequential data with revealed information, interaction, and feedback, and demonstrate their applicability for statistically framing inverse optimal control and decision prediction tasks.},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {1255-1262},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}

§Theorem 1 shows quadratic error for imitation, 

§discounted causal entropy
§https://aviationsystems.arc.nasa.gov/publications/2015/IEEE-CDC2014_Bloem.pdf



§Related Work: 
§Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards: https://arxiv.org/abs/1707.08817
§https://en.wikipedia.org/wiki/Curse_of_dimensionality

§Policy gradient theorem: (Sutton et al., 2000) 
