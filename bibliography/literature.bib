
@book{bishop,
	title={Pattern Recognition and Machine Learning},
	author={Chritopher M. Bishop},
	year={2006},
	publisher={Springer Science+Business Media, LLC}
	}

@book{Goodfellow,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@book{sutton2015reinforcement,
	title={Reinforcement Learning: An Introduction},
	author={Sutton, Richard S and Barto, Andrew G},
	year={2015},
	edition={Second edition, in progress},
	publisher={The MIT Press},
	address={Cambridge, Massachusetts, London, England},
}

@online{optimality,
    author = {Vaibhav Kumar},
    title = {Mathematical Analysis of Reinforcement Learning — Bellman Optimality Equation},
    year = {2020},
    url = {https://towardsdatascience.com/mathematical-analysis-of-reinforcement-learning-bellman-equation-ac9f0954e19f},
    urldate = {2023-02-20},
}

@article{Watkins1992,
  author  = {Watkins, Christopher J. C. H. and Dayan, Peter},
  title   = {Q-learning},
  journal = {Machine Learning},
  year    = {1992},
  volume  = {8},
  number  = {3},
  pages   = {279--292},
  doi     = {10.1007/BF00992698},
  url     = {https://doi.org/10.1007/BF00992698},
  issn    = {1573-0565},
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
}



@article{A2C,
  author    = {Volodymyr Mnih and
               Adri{\`{a}} Puigdom{\`{e}}nech Badia and
               Mehdi Mirza and
               Alex Graves and
               Timothy P. Lillicrap and
               Tim Harley and
               David Silver and
               Koray Kavukcuoglu},
  title     = {Asynchronous Methods for Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1602.01783},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.01783},
  eprinttype = {arXiv},
  eprint    = {1602.01783},
  timestamp = {Mon, 13 Aug 2018 16:47:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MnihBMGLHSK16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Inbook{proof_A,
author="Holzleitner, Markus
and Gruber, Lukas
and Arjona-Medina, Jos{\'e}
and Brandstetter, Johannes
and Hochreiter, Sepp",
editor="Hameurlain, Abdelkader
and Tjoa, A. Min",
title="Convergence Proof for Actor-Critic Methods Applied to PPO and RUDDER",
bookTitle="Transactions on Large-Scale Data- and Knowledge-Centered Systems XLVIII: Special Issue In Memory of Univ. Prof. Dr. Roland Wagner",
year="2021",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="105--130",
abstract="We prove under commonly used assumptions the convergence of actor-critic reinforcement learning algorithms, which simultaneously learn a policy function, the actor, and a value function, the critic. Both functions can be deep neural networks of arbitrary complexity. Our framework allows showing convergence of the well known Proximal Policy Optimization (PPO) and of the recently introduced RUDDER. For the convergence proof we employ recently introduced techniques from the two time-scale stochastic approximation theory.",
isbn="978-3-662-63519-3",
doi="10.1007/978-3-662-63519-3_5",
url="https://doi.org/10.1007/978-3-662-63519-3_5"
}


@online{haarnoja2018soft,
	title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
	author={Tuomas Haarnoja, et al.},
	url={arXiv:1801.01290},
	year={2018},
	urldate = {2023-02-20},
}

@misc{PPO,
  doi = {10.48550/ARXIV.1707.06347},
  
  url = {https://arxiv.org/abs/1707.06347},
  
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Proximal Policy Optimization Algorithms},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{TRPO,
  doi = {10.48550/ARXIV.1502.05477},
  
  url = {https://arxiv.org/abs/1502.05477},
  
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Trust Region Policy Optimization},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{thrun1993issues,
  title={Issues in Using Function Approximation for Reinforcement Learning},
  author={Sebastian Thrun and Anton Schwartz},
  booktitle={Proceedings of the Fourth Connectionist Models Summer School},
  year={1993},
  organization={Lawrence Erlbaum Publisher},
  address={Hillsdale, NJ}
}

@misc{TQC_Paper,
  doi = {10.48550/ARXIV.2005.04269},
  
  url = {https://arxiv.org/abs/2005.04269},
  
  author = {Kuznetsov, Arsenii and Shvechikov, Pavel and Grishin, Alexander and Vetrov, Dmitry},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{RPPO,
  doi = {10.48550/ARXIV.2205.11104},
  
  url = {https://arxiv.org/abs/2205.11104},
  
  author = {Pleines, Marco and Pallasch, Matthias and Zimmer, Frank and Preuss, Mike},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Generalization, Mayhems and Limits in Recurrent Proximal Policy Optimization},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{IL,
url = {http://dx.doi.org/10.1561/2300000053},
year = {2018},
volume = {7},
journal = {Foundations and Trends® in Robotics},
title = {An Algorithmic Perspective on Imitation Learning},
doi = {10.1561/2300000053},
issn = {1935-8253},
number = {1-2},
pages = {1-179},
author = {Takayuki Osa and Joni Pajarinen and Gerhard Neumann and J. Andrew Bagnell and Pieter Abbeel and Jan Peters}
}

@INPROCEEDINGS{InfCausalEnt,

  author={Bloem, Michael and Bambos, Nicholas},

  booktitle={53rd IEEE Conference on Decision and Control}, 

  title={Infinite time horizon maximum causal entropy inverse reinforcement learning}, 

  year={2014},

  volume={},

  number={},

  pages={4911-4916},

  doi={10.1109/CDC.2014.7040156}}

@inproceedings{NEURIPS2020_b5c01503,
 author = {Xu, Tian and Li, Ziniu and Yu, Yang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {15737--15749},
 publisher = {Curran Associates, Inc.},
 title = {Error Bounds of Imitating Policies and Environments},
 url = {https://proceedings.neurips.cc/paper/2020/file/b5c01503041b70d41d80e3dbe31bbd8c-Paper.pdf},
 volume = {33},
 year = {2020}
}
$Theorem 1 shows quadratic error for imitation, 

$discounted causal entropy
$https://aviationsystems.arc.nasa.gov/publications/2015/IEEE-CDC2014_Bloem.pdf



§Overview over reinforcement learning including from imiation: https://arxiv.org/pdf/2205.00824.pdf
§Related Work: 
$Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards: https://arxiv.org/abs/1707.08817
$https://en.wikipedia.org/wiki/Curse_of_dimensionality
