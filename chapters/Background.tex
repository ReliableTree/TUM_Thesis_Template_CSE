% !TEX root = ../main.tex

\chapter{Background}
\label{chapter:Background}
\section{Mashine Learning}
\label{section:Learning}

In this thesis we will define mashine learning as the guided search for candidate functions defined on a pre-image $X$ with image $Y$ ${f: X \rightarrow Y}$ 
using the expectation ${R = \mathbb{E}\left[g(x,f(x))\right]}$ over an objective function $g$:

\begin{equation}
    \begin{aligned}
    \label{ml_eq}
    g: X \times Y \to \mathbb{R} \\
    g(x,y) = r
    \end{aligned}
\end{equation}

Depending on the formulation $r$ can either be a reward or a loss. If the objective is to minimize $R$, it is more common to refer to it as $L$,
 indicating a loss.\\ \\
Following the book "Pattern Recognition and Machine Learning" \cite{bishop} there are three learning paradigms common in machine learning: 
\begin{itemize}
	\item Supervised Learning
	\item Unsupervised Learning
	\item Reinforcement Learning
\end{itemize} 

In supervised learning, the objective function $g$ has access to labels $y^*(x)$ for inputs $x$, from which it can compute the objective value of an output 
${y = f(x)}$. The set of input and output data to which g has access is called the training data $D$. 
\begin{equation}
    \label{supervised_paradigm}
    l = g(x, f(x)) = g(y^*(x), f(x)) | x, y^* \in D
\end{equation}
A classical example of this paradigm is image classification, where $y \in \mathbb{R}^n$ with n classes and $y_n = 1$, 0 else indicates input image $x$ shows a 
object of class n. Regression problems can also be described in this way.\\ \\

In unsupervised learning, the objective function $g$ does not have access to labels. Instead it computes a value using only the input vector $x$.
\begin{equation}
    \label{unsupervised_learning_paradigm}
    l = g(x, f(x))
\end{equation}

This paradigm can be useful to explore structure in the data by clustering or dimensionality reduction. Resent work has made use of a combination of unsupervised 
learning together with supervised- or reinforcement -learning to achieve high scores with less needed labels.\\

In both paradigms, the distribution of the data is independent of $f$. For example, the next picture you are shown does not depend on the label 
you gave to the last picture. \\ \\  

In reinforcement learning, an agent interacts with an environment and learns to make decisions based on rewards or penalties 
received from the environment. The process is defined as a sequence of actions $a_t$ acting on the environment in states $s_t$. The objective function 
$g$ in this paradigm is defined as the reward $r = g(s, a)$ or $r=g(s)$. In contrast to supervised and unsupervised learning, the data distribution 
is dependent on $f$. This means , which can be mathematically represented as follows:

\begin{equation}
\label{reinforcement_paradigm}
R = \mathbb{E}\left[\sum_{t=0}^{T} \gamma^t r_t\right]
\end{equation}

where $R$ is the expected reward, $T$ is the final time step, $r_t$ is the reward received at time step $t$, and $\gamma$ is a discount factor 
that determines the importance of immediate rewards versus future rewards. The goal of the agent is to find an optimal policy $\pi$ that maximizes 
the expected reward:

\begin{equation}
\label{policy}
\pi^* = \arg\max_{\pi} \mathbb{E}\left[\sum_{t=0}^{T} \gamma^t r_t | \pi\right]
\end{equation}

Reinforcement learning has been successfully applied in various domains, such as game playing \cite{mnih2015human}, 
robotics \cite{kober2013reinforcement}, and recommendation systems \cite{zhao2018deep}.

One common approach in reinforcement learning is to use a function approximator to estimate the value of a state or action. 
This can be achieved through deep neural networks, such as in the case of deep Q-networks (DQN) \cite{mnih2015human}, or through 
linear function approximators, such as in the case of linear value function approximation \cite{sutton2018reinforcement}. Another approach 
is to use policy gradients, where the policy is optimized directly using gradient ascent \cite{sutton2000policy}.

In summary, machine learning can be categorized into three paradigms: supervised learning, unsupervised learning, 
and reinforcement learning. Each paradigm has its own objective function and learning algorithm, and can be applied to 
various domains.

\section{Sequence Learning}
\subsection{Markov Descicion Processes}
\subsection{Partially Observable Markov Descicion Processes}
\subsection{Policy}

\subsection{SAC}
\subsection{Imiation Learning}
\subsection{Behavioural Cloning}
\subsection{Inverse Reinforcement Learning}
\section{Symmetry}
%Argue about POMDP vs. MDP from symmetry group