% !TEX root = ../main.tex

\chapter{Background}
\label{chapter:Background}
In this thesis machine learning will be defined as the guided search for candidate functions $f^*: X \to Y$ to maximize the expectation of an objective function 
${g: X \times Y \to \mathbb{R}}$ using a signal function $g^*$, which is available during training time:
\begin{equation}
    \begin{aligned}
        f^* &= \arg\max_{f} \mathbb{E}_{x \sim D}[g(x,f(x))] \\
        f^* &\approx f' = \arg\max_{f} \mathbb{E}_{x \sim D_{train}}[g^*(x,f(x))]
    \end{aligned}
\end{equation}

Commonly, $g^*$ is either a (negative) loss or a (positive) reward.

Following the book "Pattern Recognition and Machine Learning" \cite{bishop} there are three learning paradigms common in machine learning: 
\begin{itemize}
	\item Supervised Learning
	\item Unsupervised Learning
	\item Reinforcement Learning
\end{itemize} .

\section{Supervised Learning}
\label{section:super_learn}
Supervised learning is a learning paradigm where the input-output pairs $(x,y) \in X \times Y$ are given, and the goal is to learn a function 
$f^*$ that maps inputs $x \in X$ to outputs $y \in Y$. The objective function is typically a loss function that measures the difference between 
the predicted output $f(x)$ and the true output $y$.

Formally, the signal or loss function $L = g^*$ is defined on a training set $D_{train} = {(x_i,y_i)}_{i=1}^n$ where $x_i \in X$ and $y_i \in Y$. Using the 
labels, it computes a loss between the output of a function $f(x_i)$ and a desired output $y_i$. The signal function is shaped such that optimizing $f$ for $L$ 
over $D_{train}$ should be aligned with optimizing $f$ for $g$ over $D$:

\begin{equation}
\begin{aligned}
f^* &= \arg\min_{f} \mathbb{E}_{(x,y) \sim D}[g(f(x), y)] \\
f^* &\approx f' = \arg\min_{f} \mathbb{E}_{(x,y) \sim D_{train}}[L(f(x),y)] = \arg\min{f} \frac{1}{n}\sum_{i=1}^n L(f(x_i),y_i)
\end{aligned}
\end{equation}


Supervised learning can be used for a variety of tasks, including classification, regression, and sequence prediction. In classification, 
the goal is to predict a discrete class label $y \in {1,2,\ldots,K}$ for a given input $x$. In regression, the goal is to predict a continuous 
output $y \in \mathbb{R}$ for a given input $x$. In sequence prediction, the goal is to predict a sequence of outputs $y_1,\ldots,y_T$ for a 
given input sequence $x_1,\ldots,x_T$ \cite[chapter~4]{bishop} \cite[chapter~5, chapter~6]{Goodfellow}.

\section{Unsupervised Learning}
\label{section:unsup_learn}
Unsupervised learning is a learning paradigm where the input data is unlabeled and the goal is to discover underlying patterns or structure in the data.
The objective function in unsupervised learning is typically not based on a pre-defined notion of correctness, but rather on measures of statistical
independence or data compression.\\

Unsupervised learning can be used for a variety of tasks, including clustering, density estimation, dimensionality reduction, and anomaly detection.
It is also often used as a pre-processing step in supervised learning, where the goal is to learn a representation of the input data that is more informative
for the downstream task \cite[chapter~9]{bishop} \cite[chapter~5]{Goodfellow}. 

\section{Reinforcement Learning}
\label{section:rl}

Reinforcement learning is a learning paradigm where an agent interacts with an environment and learns to take actions that maximize a 
cumulative reward signal. In this paradigm, the input $x$ is typically the state $s$ of the environment, the output $y$ is the action $a$ taken by the agent, 
and the the objective function $g$ is a reward signal $r$.

The agent learns a policy $\pi: S \to A$ that maps states to actions, in order to maximize the expected cumulative reward:

\begin{equation}
    \label{rl_objective}
    \begin{aligned}
    \pi^* &= \arg\max_{\pi} \mathbb{E}_{\tau \sim p(\tau | \pi)} \left[ \sum_{t=0}^T \gamma^t r_t \right] \\
    \approx \pi' = \arg\max_{\pi} \frac{1}{N} \sum_{i=1}^N \left[ \sum_{t=0}^{T_i} \gamma^t r_{i,t} \right]
    \end{aligned}
\end{equation}

where $\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \ldots, s_T, a_T, r_T)$ is a trajectory, $p(\tau | \pi)$ is the probability of a trajectory 
under policy $\pi$, $\gamma \in [0,1]$ is a discount factor that controls the importance of future rewards, and $N$ is the number of trajectories sampled.

\subsection{Classification of Reinforcement Learning}
\begin{minipage}{\textwidth}
\begin{itemize}

    \item \textbf{Model type:}
    \begin{itemize}
    \item Model-based: the algorithm explicitly models the dynamics of the environment.
    \item Model-free: the algorithm learns a policy without explicitly modeling the environment.
    \end{itemize}
    \item \textbf{Action/Observation space:}
    \begin{itemize}
    \item Continuous: the action/observation space is continuous, typically represented by a real-valued vector.
    \item Discrete: the action/observation space is discrete, typically represented by a finite set of actions.
    \end{itemize}
    \item \textbf{Policy type:}
    \begin{itemize}
    \item On-policy: the algorithm learns the optimal policy while using the same policy to collect experience.
    \item Off-policy: the algorithm learns the optimal policy while using a different policy to collect experience.
    \end{itemize}
    \item \textbf{Reward type:}
    \begin{itemize}
    \item Dense reward: the agent receives a reward signal for every time step in the environment.
    \item Sparse reward: the agent receives a reward signal only for certain time steps in the environment.
    \end{itemize}
    \item \textbf{Time horizon:}
    \begin{itemize}
    \item Infinite horizon: the algorithm aims to maximize the expected sum of rewards over an infinite time horizon.
    \item Finite horizon: the algorithm aims to maximize the expected sum of rewards over a fixed time horizon.
    \end{itemize}
    \item \textbf{inference type:}
    \begin{itemize}
    \item Probabalistic inference: the algorithm samples actions according to a probability distribution.
    \item Deterministic inference: the algorithm chooses an action deterministicly.
    \end{itemize}
    \end{itemize}
\end{minipage}

\section{Markov Decision Processes}
Markov Decision Processes (MDPs) are a mathematical framework used to model decision-making processes that occur in a sequence. They provide a useful 
context to develop reinforcement learning algorithms.

An MDP is defined as a tuple $(S, A, T, R, \gamma)$, where:
\begin{itemize}
    \item $S$ is the set of possible states in the system
    \item $A$ is the set of possible actions that can be taken in each state
    \item $T$ is the transition function that specifies the probability of moving from one state to another given a certain action
    \item $R$ is the reward function that specifies the immediate reward obtained after taking a certain action in a certain state
    \item $\gamma$ is the discount factor that determines the importance of future rewards relative to immediate rewards
\end{itemize}

The transition function $T$ is defined as:

$$T(s, a, s') = \mathbb{P}(s_{t+1}=s' \mid s_t=s, a_t=a)$$

This function specifies the probability of moving from state $s$ to state $s'$ after taking action $a$. The reward function $R$ is defined as:

$$R(s, a) = \mathbb{E}[r_{t+1} \mid s_t=s, a_t=a]$$

This function specifies the expected immediate reward obtained after taking action $a$ in state $s$. The discount factor $\gamma$ is a 
parameter that determines the importance of future rewards relative to immediate rewards. It is typically a value between 0 and 1, where 0 means that 
only immediate rewards are important and 1 means that future rewards are equally important. \\

Importantly, the transition function $T$ satisfies the markov property. The Markov property characterizes stochastic processes where the future state of the 
process depends only on the current state and not on any past states. Formally, a stochastic process has the Markov property if, 
for all time steps $t$, states $s_t$, and actions $a_t$, the following condition holds:

\begin{equation}
    P(s_{t+1} \mid s_t, a_t, s_{t-1}, a_{t-1}, \ldots, s_1, a_1) = P(s_{t+1} \mid s_t, a_t)
\end{equation}

\section{Value Functions}

Value functions are a fundamental concept in reinforcement learning and are used to estimate the expected return of being in a particular state or taking a particular action. In this section, we will discuss the two main types of value functions in Markov decision processes (MDPs): state-value functions and action-value functions.

\subsection{State-Value Function}

The state-value function $v_{\pi}(s)$ is the expected return when starting in state $s$ and following policy $\pi$ thereafter. It is defined as:

\begin{equation}
    v_{\pi}(s) = \mathbb{E}_{\tau \propto \pi}\left[\sum_{t=0}^T \gamma^t r_t \mid s_0 = s\right].
\end{equation}

$\mathbb{E}_{\tau \propto \pi}$ denotes the expected value under policy $\pi$.

The state-value function satisfies the Bellman equation, which expresses the relationship between the value of a state and the values of its successor states:

\begin{equation}
    v_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a \mid s) \left[ r(a,s) +  \sum_{s' \in \mathcal{S}} p(s' \mid s,a) \gamma v_{\pi}(s')\right]
\end{equation}

where $\mathcal{A}$ is the set of possible actions, $\mathcal{S}$ is the set of possible states, $p(s' \mid s,a)$ is the transition probability 
from state $s$ to state $s'$ under action $a$, $r(s,a,s')$ is the reward received when transitioning from state $s$ to state $s'$ under 
action $a$, and $\gamma \in [0,1]$ is the discount factor.

\section{Action-State Value Function}

The action-state value function, also known as the Q-function, is the expected return when starting in state $s$, taking action $a$, and following 
policy $\pi$ thereafter. It is defined as:

\begin{equation}
    q_{\pi}(s, a) = \mathbb{E}_{\tau \propto \pi}\left[\sum_{t=0}^T \gamma^t r_t \mid s_0 = s, a_0=a\right]
\end{equation}

where $A_t$ is the action taken at time step $t$.

The action-state value function also satisfies the Bellman equation:

\begin{equation}
    \label{bmeq_q}
    q_{\pi}(s,a) = r(s,a) + \sum_{s' \in \mathcal{S}} p(s' \mid s,a) \left(\gamma \sum_{a' \in \mathcal{A}} \pi(a' \mid s') q_{\pi}(s',a')\right).
\end{equation}

Both value functions can be expressed in terms of the respective other value function using the Bellman equation:
\begin{equation}
    q_{\pi}(s,a) = r(s,a) + \gamma \mathbb{E}_{s'\propto T(s,a,s')}\left[ v_{\pi}(s') \right]
\end{equation}

and 

\begin{equation}
    v_{\pi}(s) = \mathbb{E}_{a \propto \pi(\cdot|s)} \left[ q_\pi(s,a) \right]
\end{equation}

\section{Optimality}

Using the reinforcement learning objective \ref{rl_objective} we can find optimality conditions for state-value and action-state value functions.

The optimal state-value function $v^*(s)$ is the maximum expected return that can be obtained from state $s$ under any policy:

\begin{equation}
v_*(s) = \max_{\pi} (v_{\pi}(s))
\end{equation}

The optimal action-state value function $q^*(s,a)$ is the maximum expected return that can be obtained from state $s$ and taking action $a$ under any policy:

\begin{equation}
q_*(s,a) = \max_{\pi} (q_{\pi}(s,a))
\end{equation}

For the action-state value function, the optimal policy is to choose actions that maximize the expected return:

\begin{equation}
    \label{impl_pi}
\pi^*(a \mid s) = \begin{cases}
1 & \text{if } a = {argmax}_{a' \in \mathcal{A}} \left( q^*(s,a') \right) \\
0 & \text{otherwise}
\end{cases}
\end{equation}

The Bellman optimality equations describe the relationship between the optimal value functions and the optimal policy:

\begin{align}
v^*(s) &=  r(s,a = \pi^*(s)) + \gamma  \sum_{s' \in \mathcal{S}} p(s' \mid s,a = \pi^*(s)) v^*(s') \\
q^*(s,a) &=   r(s,a = \pi^*(s)) + \gamma \sum_{s' \in \mathcal{S}} p(s' \mid s,a = \pi^*(s)) q^*(s',a').
\end{align}
\\ \\

\textbf{Exisitens, Uniquness and Optimality:}\\
We will briefly cover the proof of the existens and uniqueness of $v^*$. Then we will show it's optimality.   
The argument can be adapted straight forward for $q^*$. To clean up the notation, we will use upper case letters for functions, if the argument 
is implicit and lower case, if the argument is explicit. Parts of this derivation follows \cite{optimality}.\\ \\

\textbf{Definition:}\\
Let $(X, d)$ be a complete metric space. A a map $T:X \rightarrow X$ is called a contractor if there exists a factor $\gamma \in [0, 1)$ s.t.
\begin{equation}
    d(T(x), T(y)) \leq \gamma \cdot d(x,y) | x,y \in X.
\end{equation}
\\ \\

\textbf{Banach Fixed Point Theorem:}\\
Let (X,d) be a complete metric space and $T:X \leftarrow X$ a contractor. 
Then $T$ has a unique fixed point $x^* \in X$, s.t. $T(x^*) = x^*$. The sequence $lim_{n \leftarrow \infty}T^n(x)$ converges to $x^*$.\\ \\

\textbf{Value Iteration}\\
Let $X : v(s) \in \mathcal{R}$ and $d = ||X||_{\infty} = max|X|$. \\
The value iteration operator $\mathcal{B}$ is defined as:
\begin{equation}
    (\mathcal{B}v)(s) = \max_{a \in \mathcal{A}} \left[  r(s,a) +  \gamma \sum_{s' \in \mathcal{S}} p(s' \mid s,a) \left( v(s')\right) \right]
\end{equation}
\textbf{Assertion}\\

$\mathcal{B}$ is a contrator under the $\infty$-norm.\\ \\

\textbf{Proof:}\\
Let $V_1$, $V_2$ be two value functions $V : S \times A \to \mathcal{R}$, $0 \leq \gamma < 1$ and $\mathcal{B}$ the value iteration operator, then:
\begin{equation}
    \label{contractor}
||\mathcal{B}V_1(s) - \mathcal{B}V_2(s)||_{\infty, s} = 
\end{equation}
\begin{equation}
    \left|\left| \max_a \left[ r(s,a) + \gamma \sum_{s'} p(s' \mid s,a) v_1(s')\right] - \max_{a'}\left[ r(s,a') + \gamma \sum_{s'} p(s' \mid s,a') v_2(s') \right] \right|\right|_{\infty, s}
\end{equation}
\begin{equation}
    \leq \left|\left|\max_a \left[ r(s,a) + \gamma \sum_{s'} p(s' \mid s,a) v_1(s') - r(s,a) - \gamma \sum_{s'} p(s' \mid s,a) v_2(s')\right]\right|\right|_{\infty, s}
\end{equation}
\begin{equation}
= \gamma \left|\left|\max_a \left[ \sum_{s'} p(s' \mid s,a) (v_1(s') -v_2(s'))\right]\right|\right|_{\infty, s}
\end{equation}
\begin{equation}
\leq \gamma \left|\left|\max_a \left[ \sum_{s'} p(s' \mid s,a) \cdot \max_{s''} (v_1(s'') -v_2(s''))\right]\right|\right|_{\infty, s}
\end{equation}

\begin{equation}
= \gamma \left|\left|\max_{s''} \left[ (v_1(s'') - v_2(s''))\right]\right|\right|_{\infty, s}
\end{equation}

\begin{equation}
= \gamma \left|\left|(v_1(s'') - v_2(s''))\right|\right|_{\infty, s''}
\end{equation}

\begin{equation}
= \gamma \left|\left|(v_1(s) - v_2(s))\right|\right|_{\infty, s}.
\end{equation}

This leaves us to show, that the fix point $V_n$ under the value iteration operator is also the maximum of the value function $V$.\\
$\mathcal{B}$ is a monoton increasing operator on $V$: 
\begin{equation}
    \begin{align}
        \mathcal{B}v(s) = \max_{a \in \mathcal{A}} \left[ r(s,a) + \gamma  \sum_{s' \in \mathcal{S}} p(s' \mid s,a) v(s')\right]\\
        \geq r(s,a') + \gamma  \sum_{s' \in \mathcal{S}} p(s' \mid s,a') v(s') |a' \in A
    \end{align}
\end{equation}.
As $V_n$ is monoton increasing and converges to a unique value, it will approach it's supremum. 
For any practical application, this means it converges to it's maximum.

\section{Policy Gradient}
The policy in \ref{impl_pi} is derived implicitly from the maximum over the $Q$-Value. This is unfeasable for large or continuous state/action spaces. 
To approximate a policy $\pi^{\theta}$ a common approach is to use deep neural networks with parameters $\theta$. In this section we will derive a 
common approach called REINFORCE to find a good parametrisation for the policy $\pi$ as it builds the foundation of parametrized reinforcement learning. 

We start with the objective function for a policy $\pi$ as defined in Equation \ref{rl_objective}:
\begin{equation}
J(\theta) = \mathbb{E}_{\tau \sim p(\tau | \pi)} \left[ \sum_{t=0}^T \gamma^t r_t \right]
\end{equation}
where $\tau$ is a trajectory of state-action pairs, $p(\tau | \pi)$ is the probability of generating trajectory $\tau$ under policy $\pi$, and $\theta$ are the parameters of the policy $\pi$.

Our goal is to maximize $J(\theta)$ with respect to $\theta$. We can use the policy gradient theorem to compute the gradient of $J(\theta)$:
\begin{equation}
    \label{nabla_reinforce}
    \begin{aligned}
        \nabla_{\theta} J(\theta) = \mathbb{E}_{\tau \sim p(\tau | \pi)} \left[ \sum_{t=0}^T \nabla_{\theta} \log \pi(a_t|s_t;\theta) \cdot \sum_{t'=t}^T \gamma^{t'-t} r_{t'} \right]\\
        = \mathbb{E}_{\tau \sim p(\tau | \pi)} \left[ \sum_{t=0}^T \nabla_{\theta} \log \pi(a_t|s_t;\theta) \cdot  G_t\right]
    \end{aligned}
\end{equation}
With $G_t = \sum_{t'=t}^T \gamma^t' r_{t'}$ and $\pi(a_t|s_t;\theta)$ is the probability of taking action $a_t$ in state $s_t$ under policy $\pi$ with parameters $\theta$. 
$G_t$ represents the total discounted reward obtained after taking action $a_t$ in state $s_t$.

In practice we don't have access to the real expectation. Instead we sample n Trajectories and approximate the expectation.
A common choice is to sample one trajectory per update step. 
This finally gives us the gradient of the approximated expected reward with respect to the policy parameters $\theta$. We can use this gradient to update the parameters of the policy in the direction of steepest ascent. Specifically, we can use stochastic gradient ascent to update the policy parameters after observing a trajectory $\tau$:
\begin{equation}
\theta \leftarrow \theta + \alpha \nabla_{\theta} \log \pi(a_t|s_t;\theta) \cdot G_t
\end{equation}
where $\alpha$ is the learning rate. This update rule is called the REINFORCE update, and it can be used to train a policy to maximize the expected reward over a set of trajectories.

\begin{algorithm}[H]
    \SetAlgoLined
    \KwIn{Policy function $\pi_{\theta}(a|s)$, learning rate $\alpha$}
    \KwOut{Learned policy parameters $\theta$}
    Initialize policy parameters $\theta$;
    \While{not converged}{
    Sample a trajectory $\tau = (s_1, a_1, r_2, \dots, s_{T-1}, a_{T-1}, r_T, s_T)$ by following the policy $\pi_{\theta}$:\\
    \For{$t\leftarrow 1$ \KwTo $T$}{
    Compute the return starting from time $t$: \\
    $G_t = \sum_{k=t}^T \gamma^{k-t} r_k$ ;\\
    Compute the gradient of the log probability of taking action $a_t$ in state $s_t$: \\
    $\delta_{\pi(\theta)} = \nabla_{\theta}\log \pi_{\theta}(a_t|s_t)$; \\
    Compute the gradient estimate for this time step: \\
    $g_t \leftarrow G_t \delta_{\pi(\theta)}$;
    Update policy parameters using the gradient estimate:
    $\theta \leftarrow \theta + \alpha g_t$;
    }
    }
    \caption{REINFORCE algorithm with one trajectory per update}
\end{algorithm}
    
The algorithm collects 1 trajectory, computes the gradient estimate, and updates the policy parameters using stochastic gradient ascent. The algorithm continues to collect more trajectories and update the policy until convergence.

\section{Actor-Critic Algorithm}
One problem with REINFORCE is, that $G_t$ is computed from the rollout of a whole trajectory. As the policy changes with respect to the update step, this update 
rule is off policy and has a high variance. An idea is to approximate the expectation over $G_t$ using another neural network.
Revisiting \ref{nabla_reinforce}: 
\begin{equation}
    \begin{aligned}
        \mathbb{E}_{\tau \sim p(\tau | \pi)} \left[ \sum_{t=0}^T \nabla_{\theta} \log \pi(a_t|s_t;\theta) \cdot  G_t\right]\\
        = \mathbb{E}_{s_0, a_0, ... ,s_T, a_T \propto \pi} \left[
            \sum_{t=0}^{T}\nabla_{\theta} log \pi_{\theta}(a_t|s_t) \mathbb{E}_{r_t...r_T \propto \pi} \left[ G_t \right]
        \right]
    \end{aligned}
\end{equation}
, where 
\begin{equation}
    \begin{align}
        \mathbb{E}_{s_t, a_t, r_t...s_T, a_T, r_T \propto \pi} \left[ G_t \right] \\
        = r(s_t,a_t) + \gamma \mathbb{E}_{s_{t+1} \propto T(s_{t+1}|s_t, a_t)}\left[\mathbb{E}_{a_{t+1} \propto \pi(\cdot|s_t)}  \left[q_{\pi}(s_{t+1},a_{t+1})\right]  \right]
        = q_{\pi}(s_{t},a_{t}).
    \end{align}
\end{equation}
This shows the expectation over $G_t$ is the $q$-value as defined in \ref{bmeq_q}, adapted for the continuous case.

The Bellman equation for $Q$-values is defined as:
\begin{equation}
q_{\pi}(s_t, a_t) = \mathbb{E}_{\tau \propto p(\tau|\pi)} \left[ r(s_t,a_t) + \gamma q_{\pi}(s_{t+1}, a_{t+1})\right]
\end{equation}
for the sequence $\tau = (s_0,a_0, s_1, a_1...) \propto \pi$. We want to find a policy $\pi^*$ for which the $Q$-function is maximal.
\begin{equation}
    Q^* = max_{\pi} \left[\mathbb{E}_{\tau \propto \p(\tau|\pi)} \left[ r + \gamma Q_{\pi} \right]\right]
\end{equation}
From the derivation \ref{contractor} we know we only need a monotonically increasing contactor to find the $Q^*$ and thus implcitly $\pi^*$.
We can use a simlar contractor $\mathcal{B}_q$ as for the value iteration operator in \ref{contractor} where we update the Q-Value greedily:
\begin{align}
    q_{i+1}(s_t, a_t) = (\mathcal{B}_q q_i)(s_t, a_t) \\
    =\mathbb{E}_{s_{t+1} \propto p(s_{t+1}|s_t, a_t)} \left[r(s_t, a_t) + max_{a'} \left[ \gamma q_i(s_{t+1}, a_{t + 1}) \right]\right].
\end{align}
Like in \ref{contractor} this is guaranteed to find an optimal $Q$-value and thus an optimal policy $\pi^*$.

\subsection{Function Approximators}
So far, $\pi$ was computed implicitly using $Q$ as shown in \ref{impl_pi}. For large or continuous observation and action spaces, this is not feasable. Instead,
$\pi$ will be parametrized with parameters $\pi^{\theta}$, usually using deep neural networks. Similarely $Q$ is parametrized using parameters $Q^{\theta}$.
The $Q$-value can be updated using a signal function is typically a residual $J$ between the current estimate of the respective 
functions and a target y computed from the contractor $\mathcal{B}_q$ or $\mathcal{B}_v$ respectviely:
\begin{equation}
    J_Q(\theta_{i}) = \mathbb{E}_{s_t, a_t} \left[ (y_i(s_t) - Q_{\theta_i}())^2 \right]
\end{equation}