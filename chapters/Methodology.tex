% !TEX root = ../main.tex

\chapter{Methodology}
\label{chapter:Methodology}
We identify two main challenges with the single observation sparse reward setup. \\

\section{Curse of Dimensionality}
\label{COD_AC}
As introduces in \ref{COD}, the evidence for a model scales approximately inversly proportinal to the exponent of the dimension on which it acts. In MDPs, the 
actor $\pi$ acts on the observation space and returns an action from the action space. A probabalistic actor defines a probability distribution on the set 
of actions $\mathcal{A} \in \mathcal{R}^m$ and observations $\mathcal{O} \in \mathcal{R}^n$ as $\pi(a|o) = \frac{p(a,o)}{p(o)}$. 
Thus the dimension on which the actor acts on, is $n \times m$ with $\pi:\mathcal{R}^m \times \mathcal{R}^n \rightarrow \mathcal{R}$.\\
Similarely, the critic $Q$ usually defines a function on $\mathcal{A}$ and $\mathcal{O}$ to the comulative discounted expected value 
$Q:\mathcal{R}^m \times \mathcal{R}^n \rightarrow \mathcal{R}$, while TQC defines the critic as a quantile distribution: 
$Q:\mathcal{R}^m \times \mathcal{R}^n \rightarrow \mathcal{R}^k$ with $k$ quantiles.\\
We have shown in \ref{POMDP}, that in a POMDP, the observation space grows linear in time. Together with the result from \ref{COD} we conclude, that we have 
exponentially less evidence with sequence length $T$ for our model in the POMDP setting.\\
To counter this problem, we want to get rid of the time dependency of the belief state. To do this, 
we make the observation, that our case with the single observation at the beginning is a special case of a POMDP. Usually, 
we need to know all previous obervsations, as they can give us additional information about the believe state. However, as we don't have subsequent information 
after the initial observation, the only knowledge we have to include to calculate our believe state are the actions, that the policy took. As introduced in \ref{pomdp_bayes}, the optimal believe state 
update in the general POMDP case using bayes theorem is given by:
\begin{equation}
    b_{t+1}(s') = \frac{\mathcal{Z}(s', o_{t+1}) \sum_{s \in \mathcal{S}} \mathcal{T}(s, a_t, s') b_t(s)}{\sum_{s'' \in \mathcal{S}} \mathcal{Z}(a_t, s'', o_{t+1}) \sum_{s \in \mathcal{S}} \mathcal{T}(s, a_t, s') b_t(s)}
\end{equation}
. In our case, we can rewrite this as:
\begin{equation}
    b_{t+1}(s') = \frac{\mathcal{Z}(s', o_{0}) \sum_{s \in \mathcal{S}} \mathcal{T}(s, a_t, s') b_t(s)}{\sum_{s'' \in \mathcal{S}} \mathcal{Z}(s'', o_{0}) \sum_{s \in \mathcal{S}} \mathcal{T}(s, a_t, s') b_t(s)}
\end{equation}
. $\mathcal{Z}(s'', o_{0})$ is constant for $t>0$, so we get:
\begin{equation}
    b_{t+1}(s') \propto \sum_{s \in \mathcal{S}} \mathcal{T}(s, a_t, s') b_t(s) \quad | t > 0
\end{equation}
\begin{equation*}
    b_{0}(s') \propto \mathcal{Z}(s', o_{0})
\end{equation*}
. If we assume a deterministic policy $\pi(a|b) \rightarrow a_{\pi}(b)$, the update step can be written as:
\begin{equation}
    b_{t+1}(s') \propto \sum_{s \in \mathcal{S}} \mathcal{T}(s, a_{\pi}(b_t), s') b_t(s)
\end{equation}
. By using this update step recursively until we get to $b_{0}$, we see, that the believe state at time $t$ only depends on the initial observation $o_0$
the transition probability $\mathcal{T}$ and the number of recursion steps $t$. Using this insight, we can rewrite the belief state at timestep $t$ like:
\begin{equation}
    b_{t+1}(s') \propto \mathcal{Z}(s_0, o_{0}) \mathcal{T}(s_0, s', t)_{\pi}
\end{equation}
, where $\mathcal{T}(s_0, s', t)_{\pi}$ indicates that the normalized transition function depends on the timestep $t$ and policy $\pi$.\\
With this formulation, we can rewrite the policy 

\begin{equation}
    \pi(a|b) : \mathcal{R}^m \times \mathcal{R}^{n T} = \pi(a|o_0, t): \mathcal{R}^m \times \mathcal{R}^{n} \times 1.
\end{equation}
We call this a strong inductive belief state bias for the single observatino POMDP, as it forces the assumption of a deterministic and constant policy $\pi$. 
In our method, we implement a weak inductive bias, which is motivated by this observation: We use transformer encoder type architecture, which inputs are 
the repeated initial observation plus a positional encoding. We then use a single pass through the network, which means it does not work autoregressively and 
relies less on former actions to determin action at time point $t$. We test this hypothesis by using baselines that have the strong inductive bias, as well as 
using a recurrent policy.\\ 
Another benfit of computing the whole trajectory in one pass is that the independent and identically distributed (i.i.d.) input data 
assumption underlying supervised learning methods is not broken. The assumption states, that we don't expect a different data distribution durng infernece time 
then during train time, which generally does not hold for imitation learning, as the distribution of data seen during inference depends on the policy. 
However, by computing the whole trajectory at once, the input data is distributed according to the initial state of the POMDP, which is independent of 
the policy, thus the i.i.d. assumption holds.\\

\section{Update Error}
\label{inference_time_planning}
As we only have a single observation, our model has to learn the dynamics of the MDP in order to predict the reward signal at time point $t > 1$. In \cite{NEURIPS2020_b5c01503} 
it has been shown, that learning a transition model for an mdp has a quadratic error bound in $\gamma$ thus we expect quadratic error with the length of the horizon. 
However, we identify that this challange also grants us a unique improvement over previous actor critic methods. By predicting whole sequences of actions at once, 
we show that we can eliminate errors in the update step of the actor and critic.\\

\subsection{Critic}
\label{sec:AC_Critic}
To train the critic in actor critic mehtods, we can either use on policy data as in PPO \ref{section:PPO}, which is not sample efficient, as only the data collected 
in the current episode is used for training, or we can use off policy data as in TQC \ref{section:TQC}, where the critic has an overestimation bias. This 
leads to sub optimal data usage and the reliance on an addtional hyper parameter. We find that in our setup with a single observation, we can use all data 
collected without introducing a bias on the critic. \\
Recall that the overestimation bias comes from the reliance on the estimation of the $Q$ value for the bootstrapped temporal difference target \ref{eq:overestimation}:
\begin{equation}
    \label{eq:TD_Methodology}
    J_{\theta}(\pi) = \mathbb{E}_{a,a' \propto \pi, s' \propto T(s,a,s')}(r(a,s) + \gamma Q_\theta(a', s') - Q_\theta(a,s))^2
\end{equation}
, where the estimator is $Q_\theta(a', s')$. We propose to use a whole sequence critic $C_{\text{whole sequence}}$, which predicts all immediate rewards given a current observation 
and the planned action sequence for the whole trajectory:
\begin{equation}
    C_{\text{whole sequence}}(t, o, a_{1, ..., T}) = \mathbb{E}_{s_t \propto \tau_t}\left[r(a_t, s_t)\right]
\end{equation}
, where $\tau_t$ indicates the sequence until timestep $t$. 
The objective function is then defined as 
\begin{equation}
    J_\theta = \mathbb{E}_{o, a_{1, ..., t}, r_{1, ..., t}}\left[\sum_t(c_t - r_t)^2\right]
\end{equation}
. This is an unbiased estimate of the expected rewards at time step t following trajectory $a_{1, ..., t}$ and observation $o$, as it is simply the empiciral distribution 
over the rewards. Because the 
first observation is all the information we have during inference time, we find this aproach to be well suited for our setup.\\

Next we want to show that the wohle sequence critic can be used to find an optimal policy. 
To do this, we show the critic learns a proper value equvialent model of the POMDP. Recall the defining property of a proper 
value equvivalent model $m'$ for a model $m$ is defined in \ref{eq_kthVE} as:
\begin{equation*}
   \lim_{k \rightarrow \infty} \left(\mathcal{T}^{m'}_{\pi}(V)\right)^k = \lim_{k \rightarrow \infty} \left(\mathcal{T}^{m}_{\pi}(V)\right)^k
\end{equation*}
\begin{equation}
    = V_{\pi}.
\end{equation}
As we have argued above, the critic learns an unbiased estimate of the rewards $r_t$ given an action sequence. Using this, we can compute the value function for 
any action sequence $a_1, ..., a_T$ induced by a policy $\pi$:
\begin{equation*}
    V_{\pi}(s) = \mathbb{E}_{a_{1:T}, s_{1:T} \propto \pi, m}\left[\sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s\right].
\end{equation*}
As it has been shown by Christopher Grimm et. al \cite{grimm2021proper}, a policy that is optimal with respect to a proper value equivalent model $m'$ is optimal 
in the actual POMDP with model $m$. We conclude, that our whole sequence critic is suited to learn an optimal policy.

\subsection{Actor}
\label{sec:AC_actor}
All parametrized actor critic mehtods are technically speaking off policy, as the critic value is bootstrapped with estimates sampled from former policies $\pi_{old}$ during the training process. 
Once the actor is updated, the 
critic target moved, on which the actor was updated. Intuitively, when the policy is changed to choose a different action at time point $t$, it might also change 
the policy to choose different actions at 
all subsequent time points, which changes the value function. \\

We assume this update instability is specifically challenging in our one observation environment, as the policy does not 
get updates of the state of the environment it is currently in. \\
Formally, define the objective function $J(\phi, \theta(\phi)) = \mathbb{E}_{\tau \propto \pi(\phi)}\left[Q_{\theta(\phi)}\right]$ for actor critic methods 
as the function we whish to optimize to learn the optimal policy. While there are other choices, like using the value estimate $V$ instead, the general argument holds.\\
$Q_{\theta(\phi_i)}$ indicates the $Q$ function for policy $\pi_{\phi_{i}}$ parametrized by parameters 
$\theta(\phi_i)$. For example, DDPGs policy update rule \ref{AC_general_update} is written as:
\begin{equation}
    \nabla_{\phi} J(\phi, Q(\theta(\phi))) = \mathbb{E}_{\tau \sim p(\tau | \pi_{\phi})} \left[\nabla_{\phi} \log \pi(a_t|s_t;\phi) Q_{\theta, \pi_\phi}(a_t, s_t) \right].
\end{equation}
. Note that generally, we do not have access to $\theta(\phi)$, but only an approximation given the limited data we have collected. Let us denote the actual 
$Q$ function we have as $Q(\theta(\phi) + \epsilon)$, where $\epsilon$ indicates an error between the optimal parameters and the actual parameters.
Let us now for simplicity assume, that $\phi$ and $\theta$ are one dimensional. A generalisation to n dimensions is straight forward. We want 
to estimate the update error to $\pi$ from the objective $J$. The proper derivative is given by:
\begin{equation}
    \nabla_{\phi} J(\phi, \theta(\phi)) = \frac{J(\phi + \delta \phi, \theta(\phi + \delta \phi)) - J(\phi, \theta(\phi))}{\delta \phi} + \mathcal{O}(\delta)
\end{equation}
with 
\begin{equation}
    J(\phi + \delta \phi, \theta(\phi + \delta \phi)) = J(\phi, \theta(\phi)) + \left[ 
        \frac{\partial J}{\partial \phi} + \frac{\partial J}{\partial \theta} \frac{\partial \theta}{\partial \phi}
    \right] \delta \phi  + \mathcal{O}(\delta^2)
\end{equation}
, where we used the chain rule and Taylor's theorem. 
We can express this in terms of the parametrs $\theta(\phi) + \epsilon$:
\begin{equation}
    \nabla_{\phi} J(\phi, \theta(\phi)) = \nabla_{\phi} J(\phi, \theta(\phi) + \epsilon) - \nabla_{\phi} J(\phi, \theta(\phi)) + \nabla_{\phi} J(\phi, \theta(\phi))
\end{equation}
\begin{equation*}
    = \nabla_{\phi} J(\phi, \theta(\phi)) + \epsilon_{\nabla_{\theta}}
\end{equation*}
. Note that $\nabla_{\phi} J(\phi, \theta(\phi) + \epsilon)$ is not a properly defined derivative. To rigorously capture the stochastic dependence of $\theta(\phi)$ 
given a dataset $\mathcal{D}^n$ with $n$ transitions, 
we could formulate $\theta(\phi)|\mathcal{D}_n$ as n boundary conditions. Together with a suitable regularizor, we can then find $\theta_{\epsilon, n}(\phi)$. We omit 
this step, as we are only interested in the fact that there is an error, not the exact value of it.\\
In actor critic policy updates, $\theta$ is assumed a constant, thus the actor derivative 
$\widetilde{\nabla}_\phi J$ of $J$ is given by:
\begin{equation}
    \widetilde{\nabla}_\phi J(\phi, \theta) =  \frac{J(\phi + \delta \phi, \theta) - J(\phi, \theta)}{\delta \phi} + \mathcal{O}(\delta)
\end{equation}
. 
Again using $\theta(\phi) + \epsilon$ we get:
\begin{equation}
    \widetilde{\nabla}_\phi J(\phi, \theta + \epsilon) = \widetilde{\nabla}_\phi J(\phi, \theta) + \epsilon_{\widetilde{\nabla}_{\theta}}
\end{equation}
We can now derive the update error in the actor update:
\begin{equation}
    \label{equation:general_update_error}
    \nabla_{\phi} J(\phi, \theta(\phi)) - \widetilde{\nabla}_\phi J(\phi, \theta) = \nabla_{\phi} J(\phi, \theta(\phi) + \epsilon) - \epsilon_{\nabla_{\theta}} - \widetilde{\nabla}_\phi J(\phi, \theta(\phi) + \epsilon) + \epsilon_{\widetilde{\nabla}_{\theta}}
\end{equation}
\begin{equation*}
    = \epsilon_{\widetilde{\nabla}_{\theta}} - \epsilon_{\nabla_{\theta}} + \frac{\partial J}{\partial \theta} \frac{\partial \theta}{\partial \phi}
\end{equation*}
. We will call $\epsilon_{\widetilde{\nabla}_{\theta}} - \epsilon_{\nabla_{\theta}}$ the  \textbf{function approximator error}. \\
Recall that $J(\phi, \theta(\phi)) = \mathbb{E}_{\tau \propto \pi(\phi)}\left[Q_{\theta(\phi)}\right]$, so the partial derivative $\frac{\partial J}{\partial \theta} = 1$, 
as the expectation commutes with the derivative. This leaves us with estimating $\delta \frac{\partial \theta}{\partial \phi}$. Again using taylor's theorem, we get 
\begin{equation}
    \label{dist_shift_error}
    \delta \frac{\partial \theta}{\partial \phi} = \theta(\phi + \delta) - \theta(\phi) + \mathcal{O}(\delta).
\end{equation}
We will call this error the \textbf{distribution shift error}. \\
While this derivation assumes direct policy gradient to update the actor, other actor critic policy update methods like the minimisation of a KL divergence as 
proposed is SAC will rely on simlar gradients in there update step (\cite{haarnoja2018soft}, equation 13).

\subsubsection{Distribution Shift Error}
\label{dist_shift_error_section}
From \ref{dist_shift_error} we see, that the error of the update of $\pi$ is proportinal to the change in the 
expected value $J(\pi)$. Using our whole sequence critic $C$, this error is zero. \\ 


Recall when we write $\theta(\phi)$ we mean $Q_{\theta}(a,s|\pi_{\phi})$:
\begin{equation*}
    Q_{\theta}(a,s|\pi_{\phi}) = \mathbb{E}_{(a_t \propto \pi_{\phi}(s_t), s_t \propto T(s_{t-1}, a_{t-1}, s_t))|a_0=a, s_0=s}\left[\sum_t \gamma^t r(a_t, s_t)\right]
\end{equation*}
from that we get
\begin{equation}
    Q_{\theta }(a,s|\pi_{\phi + \delta}) - Q_{\theta}(a,s|\pi_{\phi}) != 0 
\end{equation}
\begin{equation*}
    \text{ if } \pi_{\phi + \delta}(a|s) != \pi_{\phi}(a|s)
\end{equation*}
except for degenerate cases. We propose a whole sequence actor, where, given an observation, the actor proposes the whole sequence of actions it will 
take. We use these as input for the whole sequence critic, as discussed in the last section. 
Recall $C_{\text{whole sequence}}$ is defined on an observation $o$ and a 
sequence of actions: 
\begin{equation}
    (a_{1,...,T}) \propto \pi_{\text{whole sequence}}(a_{1,...,T}|o)
\end{equation}
\begin{equation*}
    C_{\text{whole sequence}}(t, o, a_{1,...,T}) = \mathbb{E}_{s_t \propto \tau_t}\left[r(a_t, s_t)\right]
\end{equation*}
, where $\tau_t$ indicates the trajectory until timestep $t$ and $c_{\text{whole sequence}}(t, o, a_1, ..., a_T)$ is the expected reward at time step $t$. \\
We can now see that, as there is no dependency of $C$ on $\pi$, the distribution shift error \ref{dist_shift_error} is zero. 

\subsubsection{Function Approximation Error}
\label{func_app_error}
Recall the function approximator error $\epsilon_{\widetilde{\nabla}_{\theta}} - \epsilon_{\nabla_{\theta}}$ in \ref{equation:general_update_error} was 
derived from the observation, that our $\theta(\phi)$ is underdetermined 
given limited data $\mathcal{D}^n$. \\
Intuitively, even if the distribution shift error is zero, meaning we knew perfectly the value 
$C((t, o, a_{1,...,T}) \in \mathcal{D}^n)$ of a given action sequence, we don't know the value of an arbitrary action sequence
$C((t, o, a_{1,...,T}), o \in \mathcal{D}^n, a_{1,...,T} \notin \mathcal{D}^n)$.\\
To mitigate this error, we want to decouple the actor update from the critic by rephrasing the actor update as an imitation learning problem. 
Let's say we have $m$ expert demonstrations in our dataset. 
Like in imitation learning, we define the update step to our actor as
\begin{equation}
    \Delta \phi = \nabla_{\phi} \mathbb{E}_{(o, a_{t}) \in \mathcal{D}_E, t \in \{1, ..., T\}}\sum_n \left(\pi_{\phi}(o)_{t, n} - a_{t, n}\right)^2
\end{equation}
, where we use a whole sequence actor 
$\pi(o):\mathcal{R}^m \rightarrow \mathcal{R}^{n \times T}$ with an $n$ dimensional action space and $T$ timesteps per trajectory. The index $t$ denotes the $t$th action of the sequence and the index $n$ denotes the $n$th dimension of 
the action. For probabalistic policies, we can use the update rule as stated in \ref{prob_imitation_learning}. We don't push the actor towards a predicted goal, 
but only towards measured goals. This formulation allows us to use good lower bound on the expected policy improvement.\\

From equation \ref{eq:pol_impr_TRPO} we know our policy improvement is lower bound by the KL divergence of the policy $\pi$ before the update step and 
$\tilde{\pi}$ after the update step. We can estimate the KL divergence given our expert dataset:
\begin{equation*}  
    {\operatorname{KL}} (\pi,\tilde{\pi}) \approx \frac{1}{N} \sum_{n=1}^N {\operatorname{KL}} (\pi(a_n|s_n),\tilde{\pi}(a_n|s_n)) \quad |(a_n,s_n) \in \mathcal{D}_{\text{expert}}.
\end{equation*}  
We train $\pi$ to immitade the expert actions as close as possible. Assuming we trained $\pi$ until convergence, we get $\pi(a_n|s_n) = \tilde{\pi}(a_n|s_n)$ on seen 
data. This means our best estimate of the KL divergence between $\pi$ and $\tilde{\pi}$ is zero. No other method can do better using the emperical 
estimate of the KL divergence. The tight lower bound does not guarantee strict policy improvement, though, as it is only using emperical estimates.
\\
This leaves us with the question of how to improve our policy beyond what it can learn from the initial set of expert trajectories. To do this, we propose to use 
a search based method using the critic in inference time. Let $a_{\pi}(o)$ be the actions proposed by the actor and $c(a_{\pi}(o), o)$ be the immediate predicted rewards by the critic. 
We can now find the gradient of the critic values $c$ by the actions to define new actions: $a_{i+1} = a_i + \nabla_{a}c(a, o)$, as the predicted rewards are a 
function of the actions. This step is repeated k times, to find new actions using the critic in inference time. This usage of the critic in inference time 
is the reason, why we call this method "active critic". To be more precise, in our setup we use the critic to predict only the reward of the last state, as we are 
in the sparse reward setting. A generalisation to dense rewards is straight forward. Moreover, we don't use gradient ascent, but gradient descent on 
$J_{c, a, o} = (c(a, 0) - 1)^2$. This is a natural choice, as we can 
choose reward 1 for the success cases and reward 0 for the failure cases. In this sence, we aim to optimize the predicted probability, that actions $a$ will lead 
to a success, given observations $o$. The new trajectory $a_{c, o}$ found by unsing the critic, will 
be added to the set of trajectories for the critic and to the set of expert trajectories, if it was a success. \\
With this setup, our algorithm is truly on policy, while beeing able to use all data collected from expert demonstrations and during training time. 

The actor is model free, while the critic is weakly grounded model based, as it learns to predict the dynamics of the MDP. In this sense, it is close to MuZero, 
but our architecture enables search on continuous action space, which is not possible in MuZero. 
"Monte-Carlo Tree Search in Continuous Action Spaces with Value Gradients" does search on continues action space, but requires the knowledge of the transition model. 
To the authors knowledge, active critic is the only search based algorithm on a continuous action space with no prior access to the transition model.

\subsection{Average Actions Problem}
\label{avr_action_problem}
So far, we have assumed that the trajectories in the dataset of the actor are sampled from an expert. This implies deterministic actions, given an observation. 
In our algorithm, most trajectories in the actor dataset are the result of the search for suitable actions given the current critic. This means we could find
multiple trajectories for the same observation, given that usually multiple action sequences will successfully solve a problem. The actor is trained to miminize 
the $L_2$ distance to all trajectories with the same input $o$, as defined in \ref{actor_objective}. It is not guaranteed and in fact unlikely, that the 
average trajectory will also solve the problem. To illustrate this point, let's say our objective 
is to navigate from point A to point B around an obstacle C. Assume we found two successful trajectories 
$a_1$ and $a_2$ for observation $o$, which encodes the position of A, B and C. Given our algorithm, both trajectories are added to the actor dataset. 
The actor is now trained to minimze the $L_2$ distance, thus it will output the average of $a_1$ and $a_2$ given $o$, which might be a straight line from A to B 
through C and thus not a successful trajectory. \\
To prevent this problem, we whish to disambiguate the actions $a_1$ and $a_2$. We propose an additional network we call the planner $P$. $P$ takes as an input 
the action sequence $a$ and outputs a low dimensional encoding we call the plan $p_a$ of the sequence $a$: 
$P:\mathcal{R}^{T\times n} \rightarrow \mathcal{R}^{d_p},\ p(a) = p_a$, with action dimension $n$, sequence length $T$ and plan dimension $d_p$. This encoding is then fed into the actor together 
with the observation $o$: $a = \pi(o, p_a)$. Now the actor can learn both trajectories disjunctly. However, during inference time, we don't have access to an action sequence 
that would solve the problem to give to the planner $P$. Instead, we define the output of the planner for all action sequences that were initially provided as 
expert trajectories as the zero vector. We can do this, as long as the expert, from which the initial trajectories were sampled, is deterministic, or we make sure 
no two same observations are part of the initial expert trajectories. In inference time, we can then set the plan to the zero vector. An overview of the learning 
paradigm for the actor and planner is shown in figure \ref{fig:actor_planner}.

\begin{figure}
    \centered
    \begin{tikzpicture}
        \node[draw, fill=blue!20, rounded corners] (obs_inpt) at (0,0) {$\begin{matrix}[o_1\end{matrix} ]$};
        \node[draw, fill=blue!20, rounded corners] (obs_rep) at (2,0) {$\left[\begin{matrix}o_1\\ \vdots\\ \times T \\ \vdots \\ o_1\end{matrix}\right]_t$};
        \node[draw, thick, circle] (embplus_1) at (4, 0) {$\oplus$};
        \node[draw, thick, circle, inner sep=0pt, label={[align=left]above:Positional\\ Encoding}] 
        (pe_1) at (4,1.5) {\tikz \draw[scale=0.1] plot[domain=0.0:6.28] (\x,{sin(\x r)});};
        \node[draw, fill=blue!20, rounded corners] (seq_enc_obs_rep) at (6.5,0) {$\left[\begin{matrix}\tilde{o}_1\\ \vdots \\ \tilde{o}_T\end{matrix}\right]_t$};


        \node[draw, fill=blue!20, rounded corners] (actions) at (0,5) {$\left[\begin{matrix}a_1^E\\ \vdots\\ a_T^E\end{matrix}\right]_t$};
        \node[draw, fill=black!20, rounded corners, minimum width=2cm, minimum height=1cm] (planner) at (3,5) {Planner};
        \node[draw, fill=blue!20, rounded corners] (plans) at (6.5,5) {$\begin{matrix}[p_1, ..., p_T\end{matrix} ]^T$};

        \node[draw, fill=black!20, rounded corners, minimum width=2cm, minimum height=1cm] (actor) at (6.5,3) {$\text{Actor}_{t\rightarrow \color{black!50!green!80} t+1}$};
        \node[draw, fill=blue!20, rounded corners, right = of actor, x_shift = 1] (prop_actions) {$\left[\begin{matrix}a_1\\ \vdots\\ a_T\end{matrix}\right]_t$};
        \node[draw, thick, rectangle , right = of prop_actions, x_shift = 1] (loss) {$|a_t - a^E_t|_{_2}^{^2}$};

        \draw[->, black!50, line width=1pt] (obs_inpt) to (obs_rep);
        \draw[->, black!50, line width=1pt] (obs_rep) to (embplus_1);
        \draw[->, black!50, line width=1pt] (pe_1) to (embplus_1);
        \draw[->, black!50, line width=1pt] (embplus_1) to (seq_enc_obs_rep);
        \draw[->, black!50, line width=1pt] (seq_enc_obs_rep) to (actor);

        \draw[->, black!50, line width=1pt] (actions) to (planner);
        \draw[->, black!50, line width=1pt] (planner) to (plans);
        \draw[->, black!50, line width=1pt] (plans) to (actor);

        \draw[->, black!50, line width=1pt] (actor) to (prop_actions);
        \draw[->, black!50, line width=1pt] (prop_actions) to (loss);

        \draw[->, black!50!green!50, line width=1pt, bend right = 30, dashed] (loss.west) to (prop_actions.east);
        \draw[->, black!50!green!50, line width=1pt, bend right = 30, dashed] (prop_actions.west) to (actor.east);
        \draw[->, black!50!green!50, line width=1pt, bend right = 30, dashed] (actor.north) to (plans.south);
        \draw[->, black!50!green!50, line width=1pt, bend right = 30, dashed] (plans.west) to (planner.east);



    \end{tikzpicture}
\caption{Overview of the learning step for the actor. The solid lines indicate the forward path, the dashed lines indicate backward gradients.}
\label{fig:actor_planner}

\end{figure}

\subsection{Inference Time Search}
In the derivation of the AC algorithm, we have used the gradient signal from the critic on the actions. As our approach is completely parametrized, we can calculate 
the gradient with respect to the actor, the planner, and also the observation and the plan. In "Noether Networks: Meta-Learning Useful Conserved Quantities" \cite{https://arxiv.org/abs/2112.03321}, 
the authors aim to reconstruct a video sequence 
using conserved properties of the underlying physics. While a different problem setup, they also have a high dimensional derivative with respect to a 
generated output. The authors find, that 
building the gradient with respect to the generator and then regenerating the output performs better, then directly optimizing the output. They hypothesize this 
may be, because the generator has some knowledge about the action space. Following this insight, we found that updating the weights of the actor with respect to the 
critic gradient works best. This update rule can be written as:
\begin{equation}
    a_{i+1} = \pi_{\theta_{i+1}}(o, p(a_i))
\end{equation}
\begin{equation}
    \theta_{i+1} = \theta_i - \alpha_{inf} \nabla_{\theta} (C(\pi_{\theta}(o, p(a_i)), o) - 1)^2
\end{equation}
, where we used the planner network to compute $p(a_i)$ and $p(a_0) = 0$ for the first input to the actor network. 
We do not apply the updated weights to the actor after the inference, but only use them in inference time for the given trajectory.  An overview of the inference 
algorithm is shown in \ref{fig:inference}

\begin{figure}
    \centered
    \begin{tikzpicture}
        \node[draw, fill=black!20, rounded corners, minimum width=2cm, minimum height=1cm] (planner) at (3,0) {Planner};
        \node[draw, fill=black!20, rounded corners, minimum width=2cm, minimum height=1cm] (critic) at (6,2) {Critic};
        \node[draw, fill=black!20, rounded corners, minimum width=2cm, minimum height=1cm] (actor) at (0,2) {$\text{Actor}_{t\rightarrow \color{black!50!green!80} t+1}$};
        
        \node[draw, fill=blue!20, rounded corners] (plans) at (0,0) {
            \begin{cases}
                \begin{matrix}[p_1, ..., p_T\end{matrix} ]^\textbf{T}_{t>0}\\
                [0]_{t=0}
            \end{cases}
            };
        \node[draw, fill=blue!20, rounded corners] (actions) at (3,2) {$\left[\begin{matrix}a_1\\ \vdots\\ a_T\end{matrix}\right]_t$};
        \node[draw, fill=blue!20, rounded corners] (exp_values) at (8.5,2) {$\left[\begin{matrix}c_1\\ \vdots\\ c_T\end{matrix}\right]_t$};
        \node[draw, thick, rectangle] (loss) at (11,2) {$|c^t_T - 1|_{_2}^{^2}$};

        \node[draw, fill=blue!20, rounded corners] (obs_1) at (0,5) {$\begin{matrix}[o_1,..., \times T, ..., o_1\end{matrix} ]^\textbf{T}$};
        \node[draw, fill=blue!20, rounded corners] (obs_2) at (6,5) {$\begin{matrix}[o_1,..., \times T, ..., o_1\end{matrix} ]^\textbf{T}$};

        \node[draw, thick, circle] (embplus_1) at (0,3.5) {$\oplus$};
        \node[draw, thick, circle] (embplus_2) at (6,3.5) {$\oplus$};

        \node[draw, thick, circle, inner sep=0pt,label={[align=left]left:Positional\\Encoding}] 
        (pe_1) at (-1.5,3.5) {\tikz \draw[scale=0.1] plot[domain=0.0:6.28] (\x,{sin(\x r)});};
        \node[draw, thick, circle, inner sep=0pt,label={[align=left]left:Positional\\Encoding}] 
        (pe_2) at (4.5,3.5) {\tikz \draw[scale=0.1] plot[domain=0.0:6.28] (\x,{sin(\x r)});};


        \draw[<-, black!50, line width=1pt] (plans)  to (planner);

        \draw[->, black!50, line width=1pt] (plans)  to (actor);

        \draw[->, black!50, line width=1pt] (actor) to (actions);
        \draw[->, black!50!green!50, line width=1pt, bend right = 30, dashed] (actions.west) to (actor.east);

        \draw[->, black!50, line width=1pt] (critic) to (exp_values);
        \draw[->, black!50!green!50, line width=1pt, bend right = 30, dashed] (exp_values.west) to (critic.east);

        \draw[->, black!50, line width=1pt] (actions) to (critic);
        \draw[->, black!50!green!50, line width=1pt, bend right = 30, dashed] (critic.west) to (actions.east);

        \draw[->, black!50, line width=1pt] (obs_1) to (embplus_1);
        \draw[->, black!50, line width=1pt] (embplus_1) to (actor);
        \draw[->, black!50, line width=1pt] (actions) to (planner);

        \draw[->, black!50, line width=1pt] (pe_1) to (embplus_1);

        \draw[->, black!50, line width=1pt] (embplus_2) to (critic);
        \draw[->, black!50, line width=1pt] (obs_2) to (embplus_2);
        \draw[->, black!50, line width=1pt] (pe_2) to (embplus_2);

        \draw[->, black!50, line width=1pt] (exp_values) to (loss);
        \draw[->, black!50!green!50, line width=1pt, bend right = 30, dashed] (loss.west) to (exp_values.east);


    \end{tikzpicture}
\caption{Overview of the infernece mechanism of Active Critic. The solid lines indicate the forward path, the dashed lines indicate backward gradients, which 
are used to update the actor from timestep $t$ to \color{black!50!green!80} $t+1$ \color{black}.}
\label{fig:inference}

\end{figure}

\section{Algorithm}
The inference of the actor critic algorithm is depicted in algorithm \ref{AC_Inference}. \\

\begin{algorithm}[H]
    \caption{Active Critic Inference}
    \label{AC_Inference}
    \Require Given $\mathcal{D}_{\text{critic}}$, $\mathcal{D}_{\text{expert}}$,  
    $C_{\theta}$, $\pi_{\phi}$, $\alpha_{inf}$, $N$\\
    \State $p_a \gets 0^{T \times d_p}$ \hfill\COMMENT{Initialize plan to zero vector} \\
    \State sample $o_1$ from MDP\\
    \State $\tilde{o}_{1, ..., T} \gets [o_1]^T \oplus [p_i]_{i=1}^T$ \hfill\COMMENT{Add positional encoding $p_i$ to repeated observation} \\
    \State $a^0 \gets \pi(\tilde{o}_{1, ..., T}, p_a)$\hfill\COMMENT{Get initial actions $a^0$ according to policy $\pi$} \\
    \For {$i=1$ to $N$}{
        \State Update action $a^i$ using gradient descent:\\
        \State $a^{i} \gets a^{i-1} + \alpha_{inf} \nabla_{a^{i-1}} (c(\tilde{o}, a^{i-1}) - 1)^2$
    }
    \State Execute actions sequence $a^N$\\
    \State Get reward $r$ based on final state $s_t \propto a^N$\\
    \State Add experience to replay buffers:\\
    \State $\mathcal{D}_{\text{critic}} \gets \mathcal{D}_{\text{critic}} \cup (o_1, a^N_{1, ..., T}, r)$\\
    \If {$r = 1$}{\\
        \State $\mathcal{D}_{\text{expert}} \gets \mathcal{D}_{\text{expert}} \cup (o_1, a^N_{1, ..., T}, r)$\\
    }
    return $\mathcal{D}_{\text{expert}}$, $\mathcal{D}_{\text{critic}}$
\end{algorithm}

Following the insights from the section "curse of dimensionality" \ref{COD_AC}, we first repeat the input observation $o_1$ $T$ times and append a positional encoding $p_i$:
\begin{equation}
    \label{eq:seq_emb}
    \tilde{o}_{1, ..., T} = [o_1]^T \oplus [p_i]_{i=1}^T
\end{equation}
, with a transformer style sinusoidal encoding.
. We then generate the plan encoding as a zero vector with $p_a = 0^{T \times d_p}$ with plan dimension $d_p$. The plan and the repeated observation is the input 
of the actor $\pi$:
\begin{equation}
    a^0 = a_{\pi, 1,...,T} = \pi(\tilde{o}_{1, ..., T})
\end{equation}
. The initial action sequence $a^0$ is then optimized given the critic. For example, if we optimize the actions directly, we get:
\begin{equation*}
    a_{i+1} = a_i + \alpha_{inf}\nabla_{a} (c(o_0, a_i) - 1)^2
\end{equation*}
, with the learning rate for the inference time search $\alpha_{inf}$. Then, the trajectory is added to the respective data sets:
\begin{equation}
    \mathcal{D}_{\text{critic}} = \mathcal{D}_{\text{critic}} \cup (o_0, a_k, r_k)
\end{equation}
\begin{equation*}
    \mathcal{D}_{\text{expert}} = \mathcal{D}_{\text{expert}} \cup (o_0, a_k, r_k)\quad |\quad r_k = 1
\end{equation*}
. Note that with expert actions and expert dataset, we don't mean action sequences that were computed by 
a specific expert, for example a human, but rather we mean generally an action sequence, which solves an MDP given an observation $o$. The AC learner might add 
addtional successful trajectories to the expert dataset. \\


Now we want to derive the update step of the wohle sequence actor as motivated in the section "Actor" \ref{sec:AC_actor}.
To prevent the average action problem defined in \ref{avr_action_problem}, we first encode the expert action sequence using our planner $p_a_{\zeta} = P_{\zeta}(a^{\text{E}}_{1, ..., T})$. 
We also build the sequence embedded observation sequence $\tilde{o}_{1, ..., T}$ as described in equation \ref{eq:seq_emb}. We can now compute the actions as:
\begin{equation*}
    a_{\pi, 1,...,T} = \pi(\tilde{o}_{1, ..., T},p_a_{\zeta})
\end{equation*}
. From this we get the objective function of the actor:
\begin{equation}
    \label{actor_objective}
    J_{\phi, \zeta} = \mathbb{E}_{(o, a(t)) \in \mathcal{D}_E,\ t \in \{1, ..., T\}}\sum_n \left( \pi_{\phi}(\tilde{o}_{1, ..., T}, p_a_{\zeta})_{t, n} - a_{n}(t)\right)^2
\end{equation}
, with $n$ indicating the dimension of the action space. The respective gradient steps are:
\begin{equation*}
    \phi_{i+1} = \phi_i - \alpha_{\phi} \nabla_{\phi}J_{\phi, \zeta}
\end{equation*}
\begin{equation*}
    \zeta_{i+1} = \zeta_ - \alpha_{\zeta_} \nabla_{\zeta_}J_{\phi, \zeta}
\end{equation*}
. The pseudocode is depicted in algorithm \ref{Actor_Update_Alg}.
\begin{algorithm}
    \caption{Actor Update}
    \label{Actor_Update_Alg}
    \begin{algorithmic}
    \Require Given expert demonstrations $\mathcal{D}_{\text{expert}}$, actor $\pi_{\phi}$, planner $P_{\zeta}$, 
    learning rate $\alpha_{\phi}$, learning rate $\alpha_{\zeta}$\\
    \State Sample $(a^{\text{E}}_{1, ..., T}, o_1)$ from $\mathcal{D}_{\text{expert}}$\\
    \State $\tilde{o}_{1, ..., T} \gets [o_1]^T \oplus [p_i]_{i=1}^T$ \hfill\COMMENT{repeat observations and append positional encoding} \\
    \State $p_a \gets $
    \begin{cases}
        p_{\zeta}(a^{\text{E}}_{1, ..., T})\ |\ a^{\text{E}} \notin \text{original expert demonstrations}\\
        0^{T \times d_p}, \text{else}
    \end{cases} \hfill\COMMENT{generate plan $p_a$} \\
    \State $a^\pi_{1,...,T} \gets \pi_{\phi}(\tilde{o}_{1, ..., T}, p_a)$ \hfill\COMMENT{generate actions according to $\pi_{\phi}$} \\
    \State $J_{\pi} = \mathcal{L}_2(a^\pi_{1,...,T}, a^{\text{E}}_{1, ..., T})$ \hfill\COMMENT{compute $L_2$ loss from actions} \\
    \State $\phi \gets \phi_{old} - \alpha_{\phi} \nabla_{\phi}J_{\pi}$\\
    $\zeta \gets \zeta{old} - \alpha_{\zeta} \nabla_{\zeta}J_{\pi}$ \hfill\COMMENT{update $\phi$ and $\zeta$ according to gradients} \\
    \State return $\phi$, $\zeta$
\end{algorithmic}
\end{algorithm}
Using $\tilde{o}_{1, ..., T}$ as in \ref{eq:seq_emb}, the objective function of the whole sequence critic is defined as :
\begin{equation}
    J_{\theta} = \mathbb{E}_{(o, a_{1,...,T}, r_{1,...,T}) \in \mathcal{D}_{\text{critic}},\ t \in {1, ..., T}}\left[(c_{\theta}(\tilde{o}_{1, ..., T}, a_{1,...,T})(t) - r(t))^2\right]
\end{equation}
. The gradient step is:
\begin{equation*}
    \theta_{i+1} = \theta_i - \alpha_{\theta} \nabla_{\theta}J_{\theta}
\end{equation*}
with learning rate $\alpha_{\theta}$. The pseudocode is depicted in algorithm \ref{Critic_Update_Alg}.
\begin{algorithm}
    \caption{Critic Update}
    \label{Critic_Update_Alg}
    \begin{algorithmic}
    \Require Given replay buffer $\mathcal{D}_{\text{critic}}$, critic $C_{\theta}$ and learning rate $\alpha_{\theta}$\\
    \State Sample $(a^{\text{E}}_{1, ..., T}, o_1, r)$ from $\mathcal{D}_{\text{critic}}$\\
    \State $\tilde{o}_{1, ..., T} \gets [o_1]^T \oplus [p_i]_{i=1}^T$ \hfill\COMMENT{repeat observations and append positional encoding} \\
    \State $c \gets C_{\theta}(\tilde{o}_{1, ..., T}, a^{\text{E}}_{1, ..., T})$\hfill\COMMENT{compute expected reward $c$} \\
    
    \State $J_{C} = \mathcal{L}_2(c, r)$ \hfill\COMMENT{compute $L_2$ loss from reward $r$} \\
    \State $\theta \gets \theta{old} - \alpha_{\theta} \nabla_{\theta}J_{C}$\\
    \State return $\theta$
\end{algorithmic}
\end{algorithm}
Putting everything together, we can now define the learning paradigm of the active critic algorithm, depicted in figure \ref{AC_Learner}. The number of update 
steps in inference time $N$ can be used to weigh exploration against exploitation. A high number of update steps means the critic can find very different 
trajectories, then the one proposed by the actor. As these trajectores are further from the distribution on which the critic learned, the confidence of the 
critic score will be lower.
\begin{algorithm}
    \caption{Active critic learner}
    \label{AC_Learner}
    \begin{algorithmic}
    \Require Given expert demonstrations $\mathcal{D}_{\text{expert}}$, sequence length $T$, learning rates $\alpha_{inf}$, $\alpha_{\theta}$, 
    $\alpha_{\phi}$ and $\alpha_{\zeta}$, inference steps $N$ and update steps $N_{\text{update}}$\\
    \State Initialize  $\mathcal{D}_{\text{critic}} = \mathcal{D}_{\text{expert}}$, 
    actor $\pi_{\phi}$, planner $P_{\zeta}$, critic $C_{\theta}$\\
    \While {\text{not converged}}
    {
        \For {$i=1$ to $N_{\text{update}}$}
        {
            $\phi_{new}$, $\zeta_{new}$ = Actor Update \hfill\COMMENT{see algorithm \ref{Actor_Update_Alg}}\\
            $\theta_{new}$ = Critic Update \hfill\COMMENT{see algorithm \ref{Critic_Update_Alg}}\\
        }
        \For {$s=1$ to $N_{\text{samples}}$}
        {
           $\mathcal{D}_{\text{expert}}$, $\mathcal{D}_{\text{critic}}$ = Inference Step \hfill\COMMENT{see algorithm \ref{AC_Inference}}\\
        }
    }
    \end{algorithmic}
\end{algorithm}

\section{Dense Observations}
We now want to relax the constraint that we only use one obervsation per trajectory. To do this, we develop an addition to the active critic algorithm we call dense observation active critic (DOAC). 
We will first discuss how a straight forward adaption of AC will have an unwanted failure mode. Then we will develop an additional loss term and prove the convergence of DOAC.\\

So far we had one initial observation and repeated it $T$ times, where $T$ is the sequence length. We then used positional encoding to disambiguate the observations and generate an action sequence of 
length $T$ from it. A straight forward adaptation of this algorithm to dense observations would be to use a different encoding of the observations. We propose to use a special token to indicate an 
observation from the future that was not yet measured $t_f$. We then build the current observation sequence at time $t$ by filling the $T - t$ unseen observations with $t_f$, use positional encoding and 
the planner and 
generate the current actions sequence $a^t_{1:T} = \pi(o_{1:t}, t_{f, t+1:T}, p(a))$. Similar to the loss function of the actor from the AC algorithm, the new loss of the actor becomes: 
\begin{equation}
    \label{dense_actor_objective}
    J_{\phi, \zeta} = \mathbb{E}_{(o_{1:t}, a_{1:T}) \in \mathcal{D}_E,\ t \in \{1, ..., T\}}\left[\left( \pi_{\phi}(o_{1, ..., t}, t_{f, t+1:T}, p_{\zeta}(a))(t) - a(t)\right)^2\right]
\end{equation}
, where we now have $T$ observation sequences per trajectory, as $o_{1:t}$ and $o_{1:t+1}$ are two inputs.\\
With this construction, we can still use the whole sequence critic, 
which prediction is defined as $C(o_{1:t}, t_{f, t+1:T}, a_{1:T})(i) = \mathbb{E}_{\tau \propto a_{1:T}, o_{1:t}}\left[r(o_i, a_i)\right]$. The corresponding loss is
\begin{equation}
    J_{\theta} = \mathbb{E}_{(o_{1:t}, a_{1,...,T}, r_{1,...,T}) \in \mathcal{D}_{\text{critic}},\ t \in {1, ..., T}}\left[(c_{\theta}(o_{1, ..., t}, t_{f, t+1:T},  a_{1,...,T})(t) - r_t)^2\right]
\end{equation}

For simplicity, we will denote the prediction 
$C^t(i)$ and the actual expected reward $r(i)$. In inference time, we can choose the action sequence 
${a^t}^* = \max_{a^t} \sum_{i=1}^T C^t(i)$. We will show that with this adaptation, $C^t(i)$ is not an unbiased estimate of $r(i)$.
Let ${a^t}^*$ be the action sequence that the critic proposes at time step t, as defined above. Generally, $a^t != a^{t+1}$, as the critic sees two different input distributions 
$o_{1:t}$ and $o_{1:t+1}$. The action sequence that the AC algorithm will propose is sampled according to $a(t) = {a^t}^*(t)$. Thus the distribution of action sequences in the training data is not 
the same, as the distribution of proposed action sequences at time steps $t<T$. As the critic always chooses the action sequence that maximizes the current estimate, the estimated reward at time step 
$t$ will be an overestimation of the actual expected reward given action sequence ${a^t}^*$. To counter this, we propose the loss $J_{pred, C}$ defined as:
\begin{equation}
    \label{proxy_loss_dense}
    $J_{pred, C} = \mathbb{E}_{o_{\textbf{1:t}} , o_{t+1}, a^t_{1:T} \propto \mathcal{D}_{\text{train data}}}\left[ \sum_{i=1}^T(C(o_{\textbf{1:t}}, t_{f, t+1:T}, a^t_{1:T})(i) - C(o_{\textbf{1:t+1}}, t_{f, t+1+1:T}, a^t_{1:T})(i))^2 \right]$
\end{equation}
. Intuitively, we demand that the critic expects the same rewards at time step $t$ and $t+1$, given action sequence $a^t$. Since we always take the maximum over the estimated sum of rewads $C^t(i)$, 
the expected values at time step $t$ will converge to the actual rewards recieved, as from minimizing loss \ref{proxy_loss_dense} ensures, that the expected values at time step $t+1$ must be equal or 
higher then the expected values at time step $t$, as the action sequence ${a^t+1}^* = {a^t}^*$ can be chosen.





because if there was an action sequence with higher predicted rewards, it would have been chosen.
Assume the critic predicts at time step $t$ for action sequence $a^t$ that the reward at timepoint $t$ will be small $C(o_{1:t}, t_{f, t+1:T}, a_{1:T})(t+1) = r_{t}* \approx 0$, but 
the reward at time step $t+1$ will be high: $C(o_{1:t}, t_{f, t+1:T}, a_{1:T})(t+1) = r_{t+1}*$, with $r_{t+1}* >> 0$. Now consider action $a_t$ 
is applied with reward $r_t$ matching the expected value $r_t = r_{t}*$, as the reward at time step $t$ follows the empiciral distribution given actions including time step $t$. The new observation 
sequence is now $o_{1:t+1}$ with the new predicted reward $C(o_{1:t+1}, t_{f, t+2:T}, a_{1:T})(t+1) = r_{t+1}**$. There is no reason, why $r_{t+1}** = r_{t+1}*$, as the critic sees 
$(o_{1:t}, t_{f, t+1:T}, a_{1:T})$ and $(o_{1:t+1}, t_{f, t+2:T}, a_{1:T})$ as two different inputs. The new best action sequence $a^{t+1}$ might thus not be the same as $a^t$, and another action is 
chosen at time step $t+1$ then predicted at time step $t$. Because a different action is now chosen, the critic will not learn, that $C(o_{1:t}, t_{f, t+1:T}, a_{1:T})(t+1)$ had a wrong value. Thus 
the distribution of action sequences that the critic will be trained on is not the same, that it predicts. An approach showcased for example in {MESLI} to counter this problem is to condition the critic to predict the action sequence 
at every time step, that will be chosen eventually by the policy. This is not applicable to our case, as that would couple the critic to the actor and the strength of our approach lies in decoupling 
the two components. Thus we develop a different approach. We introduce a second loss term for the critic, which measures the $L^2$ distance between the predicted rewards at time step $t$ and $t+1$ given 
action sequence $a^t$. We will show that we have provable asymptotic optimality under a common assumption with this additional loss term. \\



\textbf{Definitions:}\\
Let $C$ be a whole sequence critic defined on $C : (n_o + n_a)^T \rightarrow \mathcal{R}^T$, with the observtion dimension $n_o$ and action dimension $a_o$. Additionally, the 
$i'th$ value of $C(x)(i)$ only depends on the input sequence until time step $i$. Also let $t_{f, t}$ be a special token indicating a not yet observed observation and $t_{f, t:T}$ be th sequence 
of $T-t$ such tokens. Finally, we will write the expectation over actions and observations at time step $t+1$ as $\mathbb{E}_{a_{1:t+1}, o_{\textbf{1:t}}}$ to indicate 
$\mathbb{E}_{a_{1:t+1}, o_\textbf{1:t}, o_{t+1} \propto (a_{1:t}, o_{1:t})}$, where we implicitly mean an expected $o_{t+1}$ according to the POMDP.
\begin{lemma}
    \label{dense_ind_start}
    $C(o_{1:t}, t_{f, t+1:T}, a_{1:T})(t)$ will converge to $\mathbb{E}_{a_{1:t}, o_{1:t}}\left[r(t)\right]$, if $C$ minimizes the $L_2$ distance to the recieved reward given the train data: \\
    $J_C = \mathbb{E}_{\mathcal{D}_{\text{train data}}}\left[(C(o_{1:t}, t_{f, t+1:T}, a_{1:T})(t) - r(t))^2\right]$, where the argument $t$ means the $t^{th}$ time step of the seqeucen.
\end{lemma}
\begin{proof}
    The reward at time step t depends on the action and state at time step t. In a POMDP, the probability distribution over the states depends on former observations and actions 
    $r_t \propto (a_{1:t}, o_{1:t})$. 
    It is known that the mean minimizes the $L_2$ distance, thus minimizing $J_C$ is equal to finding the mean of the empirical 
    distribution given the state sequence $s_{1:t}$ and action seqeuce $a_{1:t}$: 
    $C(o_{1:t}, t_{f, t+1:T}, a_{1:T})(t) = \mathbb{E}_{(s_t \propto [o_{1:t}, a_{1:T}], a_t) \propto \mathcal{D}_{\text{train data}}} \left[r(s_{t}, a_{t})\right]$. Using the law of large numbers, the empirical distribution will 
    converge to the actual distribution.
\end{proof}
\begin{lemma}
    \label{dense_ind_step}
    $C(o_{\textbf{1:t}}, t_{f, t+1:T}, a^t_{1:T})(t + 1) = \mathbb{E}_{a_{1:t+1}, o_{\textbf{1:t}}} \left[ r(t+1) \right]$ will converge to \\
    $C(o_{\textbf{1:t+1}}, t_{f, t+1+1:T}, a^t_{1:T})(t + 1) = \mathbb{E}_{a_{1:t+1}, o_{\textbf{1:t+1}}} \left[ r(t+1) \right]$, 
    if $C$ minimizes the respective $L_2$ distance given the train data: \\
    $J_{pred, C} = \mathbb{E}_{o_{\textbf{1:t}} , o_{t+1}, a^t_{1:T} \propto \mathcal{D}_{\text{train data}}}\left[ \sum_{i=1}^T(C(o_{\textbf{1:t}}, t_{f, t+1:T}, a^t_{1:T})(i) - C(o_{\textbf{1:t+1}}, t_{f, t+1+1:T}, a^t_{1:T})(i))^2 \right]$. 
    Note that one of the values depends on observations until time step $t$ and the other one depends on observations until time step $t+1$.
\end{lemma}
\begin{proof}
    The reward $r(t+1)$ at time step $t+1$ depends on the action $a_{t+1}$ and state $s_{t+1}$. In a POMDP, the probability distribution over states $s_{t+1}$ depends on $a_{1:t}$ and $o_{1:t}$, using the 
    transition function and observation function. Given this, we can now write \\
    $\mathbb{E}_{a_{1:t+1}, o_{1:t}} \left[ r(t+1) \right]$ = $\mathbb{E}_{a_{1:t+1}, o_{1:t+1}} \left[ r(t+1) \right]$. Again using the fact that the minimum over the $L_2$ distance finds the mean of 
    the distribution and the law of large numbers, we know $C(o_{1:t}, t_{f, t+1:T}, a^t_{1:T})(t + 1)$ will converge to $C(o_{1:t+1}, t_{f, t+1:T}, a^t_{1:T})(t + 1)$ if it minimizes $J_{pred, C}$ 
    over the empirical distribution $\mathcal{D}_{\text{train data}}$. 
\end{proof}
\begin{lemma}
    \label{lemma:dense_conv}
    $C(o_{1:t}, t_{f, t+1:T}, a^t_{1:T})(i)$ will converge to $\mathbb{E}_{\tau \propto o_{1:t}, a^t_{1:T}}\left[r_{i}\right]$ for $i \in {1, ..., T}$, given $C$ minimizes $J_C$ and $J_{pred, C}$. Here we mean all $T$ predictions 
    of $C$ will converge to the true $T$ expectations of the rewards given observations until time step t and the whole action sequence.
\end{lemma}
\begin{proof}
    From lemma \ref{dense_ind_start} and the fact that the critic only attends to observations and actions until time step t, we now lemma \ref{emma:dense_conv} holds for $i \seq t$. Intuitively, 
    for observations we already saw, the critic will predict the correct reward. We now want to show, that this is also the case for $i > t$. To do this, we will use induction. \\
    \textbf{Initial Case}: Let $i = t+1$:
    From lemma \ref{dense_ind_step}, we know $C(o_{\textbf{1:t}}, t_{f, t+1:T}, a^t_{1:T})(t+1) = C(o_{\textbf{1:t+1}}, t_{f, t+1:T}, a^t_{1:T})(t+1)$. using lemma \ref{dense_ind_start}, we know 
    $C(o_{\textbf{1:t+1}}, t_{f, t+1:T}, a^t_{1:T})(t+1) = \mathbb{E}_{o_{1:t+1}, a_{1:t+1}}\left[r(t+1)\right]$.
    \\
    \textbf{Induction Step}: 
    We now want to show, that if $C(o_{\textbf{1:t+1}}, t_{f, t+1:T}, a^t_{1:T})(i) = \mathbb{E}_{o_{1:t+1}, a_{1:t+1}}\left[r(i)\right]$, it is also true that \\
    $C(o_{\textbf{1:t+1}}, t_{f, t+1:T}, a^t_{1:T})(i+1) = \mathbb{E}_{o_{1:t+1}, a_{1:t+1}}\left[r(i+1)\right]$.\\
    We start by stating:\\
    \begin{equation*}
        C(o_{\textbf{1:t}}, t_{f, t+1:T}, a^t_{1:T})(i) = C(o_{\textbf{1:i}}, t_{f, i+1:T}, a^t_{1:T})(i)
    \end{equation*}


    \begin{equation*}
        = C(o_{\textbf{1:t+1}}, t_{f, t+1:T}, a^t_{1:T})(i)
    \end{equation*}
    $C(o_{\textbf{1:t}}, t_{f, t+1:T}, a_{1:T})(i+1) = \mathbb{E}_{o_{\textbf{1:t+1}}, a_{1:T}}\left[r(i+1)\right]$ also holds. \\
    From lemma \ref{dense_ind_step} we now:\\
    $C(o_{1:t}, t_{f, t+1:T}, a_{1:T})(i+1) = C(o_{1:t+1}, t_{f, t+1:T}, a_{1:T})(i+1)$. From lemma \ref{dense_ind_start} we see that 
    $C(o_{1:t+1}, t_{f, t+1:T}, a_{1:T})(i+1) = \mathbb{E}_{o_{1:t}, a_{1:T}}(r(i+1))$. This conculdes our induction step.
\end{proof}
Now back to our algorithm. We want to show, that given a critic minimizing the two proposed losses, choosing the action $a_t$ of the action sequence $a$ with the highest predicted commulative reward at 
each time step will monotonically improve the true expected value of the commulative reward of the trajectory.
\begin{lemma}
    Given a whole seqeuce critic $C$ that minimizes $J_C$ and $J_{pred, C}$, choosing the action seqeucen ${a^t}^*$ with the highest predicted reward at each time step $t$ and applying action ${a^t}^*_t$ 
    will monotonically improve the true expect value of the action sequence $a$, consisting of the respective actions $a_t = {a^t}^*_t$.
\end{lemma}
\begin{proof}
    Let's denote the true expected value $\mathbb{E}_{\tau_t, a^t}$ of the action seqeuce $a^t$ given observations $o_{1:t}$, where the first $t$ actions have already been applied. 
\end{proof}

    1. during inference time, 
    1. instead of repeating first observation T times, use it in first position, then rest with zeros.
    2. Predict Action sequence
    3. Predict reward sequence
    4. Optimize Actions sequence
    5. return action at time t

2. during training:
    1. for succesful trajectores, train actor to always propose the actions given the observations

    2. critic: train critic to predict rewards, given current observation and all actions. 
        but! magic action: the critic can propose an action at subsequent time point.
        train critic to predict same resulst before and after taking action

3. Inference time optimisation: 
    keep optimized actor and planner bu only optimize few steps. 
     
