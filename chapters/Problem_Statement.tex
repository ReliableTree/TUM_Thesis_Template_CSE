% !TEX root = ../main.tex
\chapter{Problem Statement}
\label{chapter:Problem}
In this thesis, we aim to develop an algorithm that uses search in inference time on a continuous action space. 
A focus is to efficiently leverage expert demonstrations to solve environments with sparse feedback signals. We will use fixed horizon continuous 
deterministic \ac{mdp}s with sparse rewards and fixed horizon deterministic \ac{pomdp}s, where only a single initial observation is provided, along with sparse rewards. \\

Formally we define a continuous fixed horizon \ac{sopomdp} as a tuple
$$\langle S,A,T,O,R,G, h  \rangle,$$
where $S$ is the state space, $A$ is the action space, $T$ is the transition 
function, $O$ is the observation space, $R$ is the reward function, $G$ is the set of states for which the reward is $1$, and $h$ is the time horizon.\\ 
At each time step $t=1,...,T$, the agent has access to the observation $o_t = s_1$ and selects an action $a_t$.  
It then receives a 
reward $r_t\in \{0,1\}$, where 
$\begin{cases}
    r_t = 0 \quad  |t < T\\
    r_T = 1 \quad  |g \in \{ s_t \}\\
    r_T = 0 \quad \text{else.}
\end{cases}$ \\
Intuitively, the reward is $1$ only if a goal state was reached along the trajectory. We also relax the single 
observation constraint, where the agent receives the state of the environment after each step: $o_t = s_t$, as defined for \ac{mdp}s. The rest of the setting is equal.\\

The goal of our algorithm is to learn a policy $\pi(a_t|o_{1:t}, a_{0,...,t-1})$ that maximizes the expected cumulative reward over a trajectory of length $T$ given 
some expert trajectories $D_{\text{expert}}$. 
That is, we seek to solve the following optimization problem:

\begin{equation}
\max_{\pi} \mathbb{E}_{\pi}[r_T]\quad \text{given}\quad D_{\text{expert}}
\end{equation}

where the expectation is taken over trajectories of length $T$ induced by the policy\\ $\pi(a_t|o_1, a_1, ..., a_{t-1})$.\\

Our algorithm aims to be sample efficient with respect to both expert trajectories and environment interactions. \\

Currently, \ac{gail}, a state of the art \ac{il} algorithm, has good asymptotical performance with respect to the number of expert trajectories, but is not sample efficient 
with respect to environment interactions during train time. Moreover, it does not make use of reward signals from the environment but only relies on the expert demonstrations.\\ 

An approaches to leverage expert demonstrations by sampling them with higher priority \cite{vecerik2018leveraging} has limited convergence rate towards expert behavior and is dependent on additional hyperparameters. 
An additional problem with mixing expert demonstrations and reinforcement data in actor critic algorithms is, that they induce two different distributions, one from the expert and one from 
the learner. This leads to overestimation by the critic, further hindering the learning process. \\

Apart from the expert demonstrations, critics in actor critic methods generally overestimate the expected reward. \ac{tqc} \cite{TQC_Paper} aims to solve this issue by learning the 
probability distribution of the expected reward and ignore the highest tail of it. While it shows good performance in \ac{rl} environments 
it is also reliant on additional hyperparameters 
specifying the coarseness of the learned probability distribution and the cutoff point. To solve this problem , we aim to develop a critic that is inherently unbiased using transitions from expert demonstrations 
and \ac{rl}.\\

A third difficulty with actor critic methods is that the update step of the policy must be carefully tuned. Apart from the general overestimation of the critic, it has a variance in the estimate, 
as it is a learned function approximator. The more the policy is changed in an update step, the less accurate the prediction of the critic. To solve this problem, \ac{ppo} ensures limited change of the policy 
between update steps, making it more robust. The trade-off is less exploration compared to \ac{tqc}. To have good exploration 
and ensure stable improvement without the need of extensive hyperparameter tuning, we aim to use inference time search to find and apply candidate solutions, which can vary widely from the 
proposed trajectory of the actor, without changing the actor itself. \\

While there are approaches using search in inference time, they either require discrete action spaces \cite{MUZero} or a given differentiable model of the environment \cite{Lee_Jeon_Kim_Kim_2020}. 
In our approach, we 
don't make the assumption that such a model is provided, as in real world robotic applications, this is typically not the case.\\

We are interested in investigating the effectiveness of our search paradigm, which requires a learned weakly grounded model of the environment. We first develop the approach 
for single observation environments, 
as they don't provide additional information during the trajectory is rolled out, making the prediction of the whole environment dynamics crucial. We will then relax the constraint to the typcial 
\ac{mdp} setting.