% !TEX root = ../main.tex
\chapter{Introduction}
\label{chapter:Introduction}
Deep \ac{rl} made promising advances in recent years. In particular, DeepMinds MuZero \cite{MUZero} can solve challenging problems by leveraging deep neural networks as function 
approximators and \ac{mcts} to plan actions ahead. 
Another field that made leaps forward recently is generative AI. As examples, Midjourney \cite{midjourney} 
can generate highly detailed pictures from natural 
language description and ChatGPT \cite{cite:ChatGPT} is able to provide meaningful assistance through natural language interface. We identify two important paradigms from these 
examples, which we will make effective use of: planning and pretraining. 

The planning algorithm used in MuZero is not applicable in robotics, as \ac{mcts} cannot be used, because the number of branches per node is infinite in continuous action spaces, 
making enumeration impossible. While there are recent approaches 
using planning on continuous action spaces, they either require a differentiable model of the system dynamics \cite{Manna2022} \cite{Lee_Jeon_Kim_Kim_2020}, which is difficult to provide in real world scenarios,
or are computationally costly in inference time \cite{hafner2018planet}, as they use population based methods to plan optimal action sequences.

In this thesis we develop a new \ac{rl} 
paradigm called ActiVe Critic (AVC). It learns relevant dynamics of the world model, conducts search on planned trajectories, 
and makes efficient use of expert demonstrations. Our approach is highly computation efficient in inference time. Because of the planning, it can be used with a single observation provided 
for the whole sequence, where computing the actions after the first step is just a lookup operation. It can also incorporate updates from observations during 
the trajectory, using few optimization steps per inference step to update the 
optimal trajectory. Here, we keep the knowledge from the past optimizations, making it highly efficient compared to model based methods, that start over after 
each observation.

We provide a thorough theoretical analysis of the paradigm showing improved stability in the update steps compared to actor critic methods and 
establish a lower policy improvement bound that includes expert demonstrations. 
In our experiments, we show improved sample efficiency and higher stability over two state-of-the-art general-purpose \ac{rl} algorithms, and a state-of-the-art \ac{il} algorithm. 
We conduct experiments with expert demonstrations, where we use pretraining for the \ac{rl} algorithms, and without expert demonstrations in \ac{mdp}s and \ac{sopomdp}s.

The structure of this thesis is as follows: We begin by discussing the fundamentals of \ac{rl}, followed by an overview of model-free and model-based approaches, 
including the baselines used in our study. We then introduce the \ac{il} framework, and present related literature before our problem statement. 
Next, we present our approach, detailing the key aspects of our proposed method. This is followed by our experimental results, analysis of the findings, and conclude with ideas for future research.