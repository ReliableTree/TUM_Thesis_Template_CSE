% !TEX root = ../main.tex
\chapter{Introduction}
\label{chapter:Introduction}
Deep reinfocement learning made promising advances in recent years. In particular, DeepMinds MuZero \cite{MUZero} can solve challenging problems by leveraging deep neural networks as a function 
approximators and Monte Carlo Tree Search (MCTS) to plan actions ahead. 
Another field that made leaps forward recently is generative AI. As examples, Midjourney \cite{midjourney} 
can generate highly detailed pictures from natural 
language description and "ChatGPT" is able to provide meaningful assistance through natural language interface. \\
We identify two important paradigms from these examples: search and pretraining. \\
Search is currently not widely used in robotics, as MCTS cannot be used, because the number of branches per node is infinite making enumeration impossible. While there are recent approaches 
using search on continuous action spaces \cite{Manna2022} \cite{Lee_Jeon_Kim_Kim_2020}, they require a differentiable model of the system dynamics, which is difficult to provide in real world scenarios. 
Also currently there are no vast datasets for robot trajectories public, so data availability is limited.\\ 
In this thesis we develop a new reinforcement learning 
paradigm called "Active Critic" (AC). It learns relevant dynamics of the world model, conducts search in inference time, and makes efficient use of expert demonstrations. 
To the authors knowledge, AC is the only algorithm that uses inference time search on continuous action space with no given model of the environment.\\
To test our algorithm, we choose a setup where a single observation per trajectory is provided along with a sparse reward signal and expert demonstrations for guidance. 
We also relax this constraint and test AC on an MDP setting with observations after each step.\\

We find the single observation to be an interesting setting. This setup is interesting, as it is plausible that robots may not have time to perform complex computations 
on the input signal after each time step, and sparse rewards are typically easy to define in real-world application.\\
Another possible application is natural language models. Resent efforts have been made to tune 
them according to human preferences from a pretrained initialisation \cite{cite:ChatGPT}. Language models are not relient on additional observations as the enviroment state is the 
text. The initial state is set by the user's input and the current environment state is fully determined by the the actions, respectively words, of the model. 
In this sense, it is comparable to our setup.\\


"Active Critic" is a general-purpose reinforcement learning algorithm algorithm applicable in deterministic MDPs and "Partially Observable" MDPs. 
For this reason, we compare it to two state-of-the-art general-purpose reinforcement learning algorithms, TQC and PPO. Additionally we use behavioral cloning as pretraining  
and also compare our results to GAIL, a state-of-the-art imitation learning algorithm.\\
In this thesis, we present empirical results indicating the efficacy of our algorithm in leveraging expert knowledge to significantly surpass the performance of the selected baselines in our 
experimental setup. Our findings offer compelling evidence that our algorithm is capable of swift learning and increased stability in both single observation POMDPs and MDPs when confronted with 
sparsely distributed rewards, surpassing the performance of alternative algorithms used for comparison.\\

The structure of this thesis is as follows: We begin by discussing the fundamentals of reinforcement learning, followed by an overview of model-free and model-based approaches, 
including the baselines used in our study. We then introduce the imitation learning framework and the GAIL algorithm, and present related literature before stating our problem. 
Next, we present our approach, detailing the key aspects of our proposed method. We follow this with our experimental results, analysis of the findings, and conclude with ideas for future research.