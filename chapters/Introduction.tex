% !TEX root = ../main.tex
\chapter{Introduction}
\label{chapter:Introduction}
Deep reinfocement learning made promising advances in recent years. In particular, DeepMinds MuZero \cite{MUZero} can solve challenging problems by leveraging deep neural networks as a function 
approximators and Monte Carlo Tree Search (MCTS) to plan actions ahead. 
Another field that made leaps forward recently is generative AI. As examples, Midjourney \cite{midjourney} 
can generate highly detailed pictures from natural 
language description and "ChatGPT" is able to provide meaningful assistance through natural language interface. \\
We identify two important paradigms from these examples: search and pretraining. In this thesis, we develop the "Active Critic" (AC) algorithm, which aims to effectively use 
expert demonstrations and inference time search given a single observation per trajectory. 
We will also develop and test a version of Active Critic called "Dense Observation Active Critic" which acts on Marcov Descision Processes (MDP), getting an observation after each step. With this setup we 
demonstrate that our approach is adaptable to a wide range of applications.\\ \\
Search is currently not widely used in robotics, as MCTS cannot be used, because the number of branches per node is infinite making enumeration impossible. While there are recent approaches 
using search on continuous action spaces \cite{Manna2022} \cite{Lee_Jeon_Kim_Kim_2020}, they require a differentiable model of the system dynamics, which is difficult to provide in real world scenarios. 
Also currently there are no vast datasets for robot trajectories public, so data availability is limited.\\ 
In this thesis we develop a new reinforcement learning 
paradigm called "Active Critic" (AC). It learns relevant dynamics of the world model and conducts search in inference time, which we find to be a key aspect 
to make efficient use of expert knowledge. To the authors knowledge, AC is the only algorithm that uses inference time search on continuous action space with no given model of the environment.\\

To test our algorithm, we choose a setup where a single observation per trajectory is provided along with a sparse reward signal and expert demonstrations for guidance. \\

We find this to be an interesting setting. While a single observation is not a typical assumption in current reinforcement learning, it is plausible that robots 
will not have time to do complex computation on the input signal after each time step. Additionally sparse rewards are a practical constraint. 
They are typically easy to define even in real-world applications. Alternatively sparse reward from human feedback could be used. It is well 
discussed that reward shaping is tedious and can lead to sub optimal performance. As an example of our setup, consider an automated production line, in which a robot performs a quick 
correction step, like cutting off bad parts of food at high speed based on an image of the object. A human could easily provide the feedback as to whether 
the object is in the intended state after the operation. It would be vastly more effort to provide a reward signal for every step along the trajectory. Moreover it is 
non-trivial to do so in a way that is helpful for the learning algorithm. Since the time to perform the trajectory is short, only a sparse set of images could be processed per trajector.\\
Another application are natural language models. Resent efforts have been made to tune 
them according to human preferences from a pretrained initialisation \cite{cite:ChatGPT}. Language models are not relient on additional observations as the enviroment state is the 
text. The initial state is set by the user's input and the current environment state is fully determined by the the actions, respectively words, of the model. 
In this sense, it is comparable to our setup.\\


"Active Critic" is applicable in deterministic MDPs and "Partially Observable" MDPs, 
so it can be adapted to a wide range of use cases. 
For this reason, we compare it to two state-of-the-art general-purpose reinforcement learning algorithms, namely TQC and PPO. Additionally we use behavioral cloning as pretraining  
and also compare our results to GAIL, a state-of-the-art imitation learning algorithm.\\
In this thesis, we present empirical results indicating the efficacy of our algorithm in leveraging expert knowledge to significantly surpass the performance of the selected baselines in our 
experimental setup. Our findings offer compelling evidence that our algorithm is capable of swift learning and increased stability in both single observation POMDPs and MDPs when confronted with 
sparsely distributed rewards, surpassing the performance of alternative algorithms used for comparison.\\

The structure of this thesis is as follows: We begin by discussing the fundamentals of reinforcement learning, followed by an overview of model-free and model-based approaches, 
including the baselines used in our study. We then introduce the imitation learning framework and the GAIL algorithm, and present related literature before stating our problem. 
Next, we present our approach, detailing the key aspects of our proposed method. We follow this with our experimental results, analysis of the findings, and conclude with ideas for future research.