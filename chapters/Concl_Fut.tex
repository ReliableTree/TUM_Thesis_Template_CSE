% !TEX root = ../main.tex

\chapter{Conclusion and Future Work}
\label{chapter:Conc_Fut}
In this thesis, we have developed a novel general approach for reinforcement learning in deterministic \ac{mdp}s and \ac{pomdp}s called the \ac{avc} algorithm, 
which effectively employs search on continuous action spaces and utilizes 
expert demonstrations to solve sparse reward \ac{rl} tasks. Our results demonstrate that \ac{avc} 
outperforms state-of-the-art general-purpose \ac{rl} algorithms in a variety of robot manipulation tasks, achieving superior performance 
while maintaining high sample efficiency. We provided evidence that search is a critical component that enables us to make the best use of available 
environment interactions.

\section{Future Work}
\subsection{Indeterminacy}
Our current search paradigm does not account for the various possible future observations, which poses a significant challenge in 
continuous observation and action spaces where enumeration is impossible. To address this issue, we propose incorporating a stochastic 
representation of observations to sample from a probability density estimate provided by the critic. 
This approach could prove promising in developing an efficient search paradigm in such environments.

\subsection{Uncertainty Estimate}
Modelling an uncertainty estimate could be another crucial aspect of improving our search paradigm. 
Currently, the critic searches for solutions with a high expected probability without considering uncertainty. 
Incorporating an uncertainty estimate could assist the critic in finding solutions that maximize expected reward with high confidence. 
Existing uncertainty-aware approaches \cite{gawlikowski2022survey,liu2022simple} 
could be integrated into the \ac{avc} algorithm.

\subsection{Curiosity Driven Exploration}
To further enhance exploration, we could incorporate curiosity-driven exploration approaches \cite{pathak2017curiositydriven}. 
Curiosity provides an intrinsic reward that could be specifically useful in sparse reward settings. 
Currently, the \ac{avc} algorithm selects a provided trajectory if the critic estimates that it solves the environment. By incorporating curiosity, 
we can explore more robust solutions or gain a better understanding of \ac{mdp} dynamics by modifying trajectories even in environments that the 
algorithm can already solve. Here, incorporating an uncertainty estimate could be beneficial as well.