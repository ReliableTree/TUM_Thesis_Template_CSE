% !TEX root = ../main.tex

\chapter{Discussion}
\label{chapter:Discussion}
In this chapter, we will summarize our findings and discuss advantages as well as remaining limitations of our approach and evaluation.
\section{Applicability and Advantages}
\subsection{Imitation Learning}
Our results from Section \ref{sec:exp_imi_lr} demonstrate that our \ac{il} approach outperforms recurrent algorithms in continuous action space, 
given a single observation per trajectory. We propose this is due to generating the whole sequence in one step, 
rather than using an autoregressive approach. This method does not break the \ac{iid} assumption, making predictions more robust and effective 
for planning in continuous space. 

\subsection{Optimization Mode}
In Section \ref{ref:com_opt_modes}, we conducted an analysis of various optimization modes employed in the \ac{avc} algorithm. 
Our experiments revealed that the optimization mode plays a critical role in determining the \ac{rl} performance. 
Specifically, we propose leveraging the actor to encode a useful space of candidate solutions that the critic can search on. 
This approach allows for informed search and leads to faster convergence and more efficient exploration of candidate solutions. We take the strong effect of the inference time 
search paradigm on the convergence rate of the algorithm as evidence for the importance and effectiveness of our use of inference time search.

\subsection{Reinforcement Learning}
We explored the effectiveness of \ac{rl} in improving the performance of our approach as compared to pure \ac{il}. 
Our results in Section \ref{sec:fine_tuning} demonstrate that our approach can effectively leverage both expert demonstrations and \ac{rl} data to achieve 
superior performance compared to our baselines.

To further assess the impact of \ac{rl}, we conducted experiments on the Drawer Close environment without expert demonstrations. 
In this environment, a solution can be achieved through random trajectories to some degree. 
Our analysis revealed that our algorithm outperformed the baselines significantly in this pure reinforcement setting, 
indicating that our approach is more stable with sparse rewards than traditional actor critic methods.




\subsection{MDP Setting}
In Section \ref{sec_exp_con_obs}, we evaluated the performance of the \ac{coavc} approach in the standard \ac{mdp} setting, 
where observations are available after each step. Our experiments demonstrated that our method significantly outperforms the baselines, making efficient use of expert demonstrations and reinforcement learning.

To evaluate the performance of pure \ac{rl} in \ac{mdp}s, we conducted experiments on the Drawer Close 
environments without expert demonstrations. Our results show that all baselines and \ac{coavc} can find solutions, but \ac{coavc} is more stable and achieves 
faster convergence. These findings provide further evidence of the general applicability and better performance of our approach over the baselines. 

\subsection{Single Observation and MDP Comparison}
We compared \ac{coavc} to \ac{avc} in Section \ref{sec:com_coavc_avc} and observed that while \ac{coavc} performs better, 
\ac{avc} already achieves high accuracy in solving environments. We attribute this to the structured exploration enabled by knowledge from 
expert data. Moreover, the inference of \ac{avc} is \ac{iid}, while that of \ac{coavc} is not, as observations at inference time are dependent on actions. 

In conclusion, our experiments demonstrate that the \ac{avc} paradigm is highly stable and performs well in settings with sparse rewards. 
It effectively leverages expert demonstrations for both exploitation and informed search. While \ac{coavc} provides improved performance over 
AVC, the latter already achieves high accuracy in solving environments.

\section{Limitations and Drawbacks}
\subsection{Baselines}
We have chosen general-purpose \ac{rl} algorithms for our baselines, because our paradigm is also generally applicable 
and we wanted to test it in a wide range of experiments. This means however, that the baselines are not specifically tailored for fine-tuning or sparse rewards. 

We think it is interesting to test the specific strength in fine-tuning against baselines with priority replay buffers as introduces in Section \ref{sec:rel_work_finetuning}. 
While we argued why we expect our algorithm to perform better in cases with provided expert demonstrations than the featured algorithm, we have to 
leave testing it for future work.

Another aspect that we have not tested in more depth is the sparse reward setting. We have introduced two concepts in Section \ref{sec:HER}, that are used for sparse \ac{rl}, namely 
\ac{her} and curiosity driven exploration. Both approaches are not applicable in the \ac{sopomdp} setting, as they rely on observations during the trajectory. 
They are applicable in the context of the \ac{mdp} setting though, and it should be tested adding them to \ac{avc}, \ac{ppo} and \ac{tqc}.

\subsection{Exploration}
Although \ac{coavc} outperformed baselines, its performance in the Pick and Place and Push environments did not reach near-perfect levels, 
indicating limitations in more challenging environments. While a longer reinforcement phase might improve performance, testing this is left for 
future work. \ac{avc} currently does not use a stochastic actor, which we expect to be key for improved exploration.

Additionally, exploration from no expert demonstrations was not extensively studied, except for the Drawer Close 
environment. We expected pure \ac{rl} is not feasible for more challenging environments given sparse rewards. 
However, we found \ac{tqc} was able to solve the Window Open environment without expert demonstrations, but further investigation on whether 
\ac{coavc} performs well in this environment was not possible due to time constraints.

\subsection{Indeterminacy}
In \ac{coavc}, the critic estimates the expected reward given an action sequence and chooses the best sequence greedily. 
However, choosing the action sequence with the highest expected reward is not equivalent to choosing the action with the highest posterior 
expectation over both the environment dynamics and action sequences. In indeterministic environments, the optimal action sequence may depend on later observations. To 
search for the best current action in an indeterministic \ac{mdp} requires the estimate of multiple possible observations and action sequences, respectively.

In the current version, planning with multiple possible action sequences that can depend on later observations is not possible. 
Although \ac{coavc} can update the action sequence given new observations, it cannot plan for multiple possible future observations.