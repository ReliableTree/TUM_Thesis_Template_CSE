% !TEX root = ../main.tex

\chapter{Discussion}
\label{chapter:Discussion}
In this chapter we will summarize our findings and discuss advantages as well es remaining limitations of our approach.
\section{Applicability and Advantages}
\subsection{Imitation Learning}
Our results demonstrate that our imitation learning approach outperforms recurrent algorithms in continuous action space, 
given a single observation per trajectory. This is due to the encoding of a sequence from which we generate a sequence in one step, 
rather than using an autoregressive approach. This method does not break the i.i.d. assumption, making predictions more robust and effective 
for planning in continuous space. \\
\subsection{Reinforcement Learning}
We explored the effectiveness of reinforcement learning in improving the performance of our approach as compared to pure imitation learning. 
We demonstrate that our algorithm can effectively utilize data from reinforcement learning, as evidenced by our results presented in 
Section \ref{sec:fine_tuning}.\\
To further assess the impact of reinforcement learning, we conducted experiments on the "Drawer Close" environment without expert demonstrations. 
In this environement, a solution can be achieved through random trajectories to some degree. 
Our analysis revealed that our algorithm outperformed the baselines significantly in this pure reinforcement setting, 
indicating that our approach is more stable with sparse rewards than traditional actor-critic methods.\\
Our results demonstrate that our approach can effectively leverage both expert demonstrations and reinforcement learning data to achieve 
superior performance in the single observation sparse reward POMDP setup.

\subsection{Optimisation Mode}
In Section \ref{ref:com_opt_modes}, we conducted an analysis of various optimization modes employed in the active critic algorithm. 
Our experiments revealed that the optimization mode plays a critical role in determining the reinforcement learning performance. 
Specifically, we propose leveraging the actor to encode a useful space of candidate solutions that the critic can search on. 
This approach allows for informed search and leads to faster convergence and more efficient exploration of candidate solutions.

\subsection{MDP Setting}
In Section \ref{sec:DO_exp}, we evaluated the performance of the continued observation active critic" (COAVC) approach in the standard MDP setting, 
where observations are available after each step. Our experiments demonstrated that our 
method significantly outperforms the baselines. Furthermore, we observed that TQC benefits greatly 
from the MDP setting, as it is more efficient at exploration then PPO. This was a disadvantage in the single observation setting, as with no feedback 
from the environement a less predictable behaviour of an algorithm makes the estimate of the current state of the environment less accurate.\\
To evaluate the performance of pure reinforcement learning in MDPs, we conducted experiments on the "Drawer Close" 
environments without expert demonstrations. Our results show that all baselines and COAVC can find solutions, but COAVC is more stable and achieves 
faster convergence. These findings provide further evidence of the general superior performance of our approach over the baselines. 

\subsection{Single Observation and MDP Comparison}
We compared COAVC to AVC in section \ref{sec:comp_dense_sparse} and observed that while COAVC performs better, 
AC already achieves high accuracy in solving environments. We attribute this to the structured exploration enabled by knowledge from 
expert data. Moreover, the inference of AVC is i.i.d., while that of COAVC is not, as observations at inference time are dependent on actions. 

In conclusion, our experiments demonstrate that the AVC paradigm is highly stable and performs well in settings with sparse rewards. 
It effectively leverages expert demonstrations for both exploitation and informed search. While COAVC provides improved performance over 
AC, the latter already achieves high accuracy in solving environments.

\section{Limitations and Drawbacks}
\subsection{Exploration}
Although COAVC outperformed baselines, its performance in the "Pick and Place" and "Push" environments did not reach near-perfect levels, 
indicating limitations in more challenging environments. While a longer reinforcement phase might improve performance, this was not explored 
due to time constraints. Additionally, exploration from no expert demonstrations was not extensively studied, except for the "Drawer Close" 
environment, as we expected pure reinforcement learning is not feasable for more challenging environments given sparse rewards. 
However, we found TQC was able to solve the "Window Open" environment without expert demonstrations, but further investigation on whether 
COAC performs well in this environment was not possible due to time constraints. It is expected that COAVC would be less proficient in exploration 
compared to TQC from no expert data, as COAVC currently lacks a stochastic actor and does not use entropy regularization, which has been shown to 
be crucial for exploration. Therefore, incorporating these features into the COAVC algorithm is key for future improvements.

\subsection{Indeterminacy}
In COAC, the critic estimates the expected reward given an action sequence and chooses the best sequence greedily. 
However, choosing the action sequence with the highest expected reward is not equivalent to choosing the action with the highest posterior 
expectation over both the environment dynamics and action sequences, as the optimal action sequence may depend on later observations. To 
search for the best current action in a non-deterministic MDP requires the estimate of possible observations and action sequences, respectively. 
In the current version, planning with multiple possible action sequences that can depend on later observations is not possible. 
Although COAVC can change the action sequence given new observations, in deterministic MDPs, the greedy action sequence with respect to expected 
reward is the best possible action sequence given a current state and a perfect model of the environment. This is why COAVC performs well in 
deterministic MDPs such as the Meta-World benchmark used in this study.