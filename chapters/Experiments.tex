% !TEX root = ../main.tex

\chapter{Experiments}
\label{chapter:Experiments}
The goal of our experimental evaluation is to understand how the two main ideas of our algorithm contribute to its perforance in one observation, obscure goal 
POMDPs, namely the weak inductive bias as developed in \ref{COD_AC} and the inference time planning developed in \ref{inference_time_planning}. 
To do this, we first analyze our weak inductive bias against a proposed method using a recurrent architecture. 
We directly compare our results on the benchmark they provided. Second, we evaluate the perforance difference between pure imitation learning and reinforcement 
learning with expert demonstrations on the reach environment from the Meta-World benchmark. Here we compare our inference time planning against two state of 
the arts reinforcement learning algorithms, which are pretrained using behavioural cloning. For our third experiment setup, we evaluate our approach against 
baselines on 5 tasks from the Meta-World benchmark. As one observation POMDPs are not well studied, we propose a variety of  
baselines aiming to challenge aspects we developed in our methodology.


\section{Imitation Learning}
In this section we present our findings on the natural language conditioned robot manipulation task benchmark, that was propsed by Simon Stepputtis et.al 
\cite{stepputtis2020languageconditioned}. The benchmark consists of a 7 dof simulated robot arm with a table-top setup using CoppeliaSim which allows 
for accurate dynamics simulations at an update rate of 20Hz. The task is to pick up the correct cup and pour the content into the correct bowl, using 
natural language description of the cup and bowl and an RGB picture of the scene.\\
Three differently colored cups containing a granular material which could be poured into the bowls were used. For the bowls, 20 variations of in two sizes, 
two shape types, and five colors are used.\\
Successful picking action involved lifting a grasped object stably from the table, while successful pouring was detected whenever the cup's dispersed 
content ended up in the correct bowl. A random subset of objects was placed on the table, with a constraint to prevent collisions or other artifacts. 

Figure \ref{lang_imi_expl} depicts the table-top setup and the different variations of the objects used, as well as an example task.

As stated in section \ref{LCILRM}, in this experiment only the first observation consisting of the RGB values from the picture of the scene and the 
task description is visible for the actor. With this setup we want to evaluate our positional encoding bias. The method proposed in the accompanying 
paper "Language-Conditioned Imitation Learning for Robot Manipulation Tasks" uses a recurrent policy which we will refer to as LCIL. 
As we have motivated in section \ref{COD_AC}, 
we expect our method to perform better then autoregressive or recurrent models, because we use the knowledge that we are not getting additional information 
from the environment during the unroll of the trajectory. To test this, we have used the dataset of 40000 expert demonstrations provided 
with the benchmark to train our model using only imitation learning. 

The results are depicted in table

\begin{table}
    \centering
    \caption{Example table}
    \begin{tabular}{|c|c|c|}
        \cline{2-3}
        \multicolumn{1}{c|}{} & \textbf{Active Critic} & \textbf{LCIL} \\ \hline
        \textbf{Pick} & 100 \% & 98 \% \\ \hline
        \textbf{Pour} & 99 \% & 95 \% \\ \hline
        \textbf{Combined} & 98 \% & 92 \% \\ \hline
    \end{tabular}
    \caption{Success rates of the 100 pick, pour and combined tasks. The left column depicts the results from the Active Critic algorithm (ours) in 
    imitation mode. Right is the result of the LCIL architecture.}
\end{table}

On the 100 test environments that were provided, we decreased the overall error rate by 75$ \% $. 
\begin{figure}
    \captionsetup[subfigure]{justification=Centering, labelformat=empty}
    \begin{subfigure}[t]{0.18\textwidth}
        \includegraphics[width=\textwidth]{images/Language_Conditioned_Exp/theirs_1.png}
        \caption{Time step 60.}
    \end{subfigure}
    \begin{subfigure}[t]{0.18\textwidth}
        \includegraphics[width=\linewidth]{images/Language_Conditioned_Exp/theirs_2.png}
        \caption{Time step 120.}
    \end{subfigure}
    \begin{subfigure}[t]{0.18\textwidth}
        \includegraphics[width=\linewidth]{images/Language_Conditioned_Exp/theirs_3.png}
        \caption{Time step 180.}
    \end{subfigure}
    \begin{subfigure}[t]{0.18\textwidth}
        \includegraphics[width=\linewidth]{images/Language_Conditioned_Exp/theirs_4.png}
        \caption{Time step 240.}
    \end{subfigure}
    \begin{subfigure}[t]{0.18\textwidth}
        \includegraphics[width=\linewidth]{images/Language_Conditioned_Exp/theirs_5.png}
        \caption{Time step 300.}
    \end{subfigure}

    \bigskip % more vertical separation
    \begin{subfigure}[t]{0.18\textwidth}
        \includegraphics[width=\linewidth]{images/Language_Conditioned_Exp/mine_1.png}
        \caption{Time step 60.}
    \end{subfigure}
    \begin{subfigure}[t]{0.18\textwidth}
        \includegraphics[width=\linewidth]{images/Language_Conditioned_Exp/mine_2.png}
        \caption{Time step 120.}
    \end{subfigure}
    \begin{subfigure}[t]{0.18\textwidth}
        \includegraphics[width=\linewidth]{images/Language_Conditioned_Exp/mine_3.png}
        \caption{Time step 180.}
    \end{subfigure}
    \begin{subfigure}[t]{0.18\textwidth}
        \includegraphics[width=\linewidth]{images/Language_Conditioned_Exp/mine_4.png}
        \caption{Time step 240.}
    \end{subfigure}
    \begin{subfigure}[t]{0.18\textwidth}
        \includegraphics[width=\linewidth]{images/Language_Conditioned_Exp/mine_5.png}
        \caption{Time step 300.}
    \end{subfigure}
    \caption{Comparison of a pour task. The policy learned by the LCIL algorithm (top row) spills the content over the table and moves unpredictable. The task is considered a failure 
    by the benchmark, as most drops did not land in the goal bowl. Active Critic (bottom row) performs the task as intendet with no unexpected movement.}
    \label{fig: AC vs. Rec}
\end{figure}
Moreover in figure \ref{fig: AC vs. Rec} we have depicted a case in the test dataset, which our policy solved and compared it to the performce of the recurrent algorithm, which failed at the task. 
While our approach hit the target precisely, the recurrent model was widely off. We assume this is caused by the fact that during inference time, the 
i.i.d. assumption of the input data to the policy is broken. Intuitively, the recurrent policy 
is in a state that it has not seen before and acts slightly different to the expert movement. After some steps, the policy now sees inputs that are vastly 
different then the training distribution, thus it starts to act unpredictable. As argued in section \ref{COD_AC} this is an advantage of our policy, as it 
does not break the i.i.d. assumption.

\section{Meta World}
The Meta World benchmark is a challenging robot simulation benchmark...

We want to test two aspects of our algorithm
-fine tuning
-reinforcement learning

RPPO TQC PPO fine tuning, positional encoding, reasons why they were chosen.
We tested our algorithm on four environments from the Meta-World benchmark with variying complexity. 
PPO, TQC and RPPO were pretrained unsing behavioural cloning. We chose the best performing policy during behavioural cloning by testing the policy on 50 test 
trajectories every 400 full cycles through the 
expert transitions. Technically, these test trajectories were environment interactions, but we chose not to count them to give the best case comparison to the AC algorithm, which was not pretrained. 


\subsection{Fine Tuning}
\label{sec:fine_tuning}
In this section we analyze our algorithm in a fine tuning setup. We provided the algorithms with 4 and 15 expert demonstrations per environment and used a relatively short reinforcement learning phase 
with 400 sampled trajectory consisting of 100 steps each. One problem we wanted to overcome is "catastrophic 
forgetting" that can occure, when the data distribution shifts. During the imitation phase, the learners only sees positive examples sampled according to the distribution induced by the expert. 
In the reinforcement learning phase, the learner samples trajectories according to the actor of the learner, thus the distribution shift. Our algorithm is more robust to this problem, as it 
decouples the generator distribution, namely the distribution induced by the actor, from the training process, as discussed in section \ref{inference_time_planning}. We present our findings in 
figure \ref{fig:finetuning}. For the environments "Pick and Place" and "Push" we show the plots 
for 15 expert demonstrations, as we found 4 expert demonstrations were not enough to meaningfully learn within 400 training episodes. The environments "Reach" and "Window Open" are easier to learn, 
so we display the experiments with 4 expert demonstrations each, as giving 15 expert demonstrations lead to near pefect performance from imitation learning alone. All plots for all experiments can 
be found in Appendix \ref{chapter:additional_plots}. 
We find, that AC is the only algorithm that can meaningfully improve it's perforance given the limited amount of environment interactions. RPPO is a recurrent algorithm and is the most sensitive to hyper parameter tuning. 
We expected RPPO to be the least performant from our discussion of the curse of dimensionality in section \ref{COD_AC}. The algorithms guided by GAIL did not improve meaningfully. We assume the limited 
amount of environment interaction was not proficcient to learn a good discriminator and improve the policies. We tried differnt hyper parameter settings, including fine tuned parameters for the 
environments but for dense rewards and observations from the [hyper parameter ZOO]. We found that the performance was most relient on the learning rate. Too high values lead to catastrophic forgetting. Some cases can be seen in 
the appendix \ref{chapter:additional_plots}, where we were not able to find suitable hyper parameters. The values we chose were the highest values before pratically no learning took place, however 
mostly the performance decreased, anyway.


\begin{figure}[htbp]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
      \includegraphics[width=\textwidth]{images/FineTuning/Window Open.png}
      \caption{Window Open environment, 4 expert demonstrations.}
      \label{fig:plot3}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
      \includegraphics[width=\textwidth]{images/FineTuning/Reach.png}
      \caption{Reach environment, 4 expert demonstrations.}
      \label{fig:plot1}
    \end{subfigure}
    \medskip
    \begin{subfigure}[t]{0.45\textwidth}
      \includegraphics[width=\textwidth]{images/FineTuning/Push.png}
      \caption{Push environment, 15 expert demonstrations.}
      \label{fig:plot2}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
      \includegraphics[width=\textwidth]{images/FineTuning/Pick and Place.png}
      \caption{Pick and Place environment, 15 expert demonstrations.}
      \label{fig:plot4}
    \end{subfigure}
    \caption{
    The learner trained by GAIL and RPPO were pretrained using behavioural cloning with the given number of expert demonstrations. 
    The x-axis shows the number of sampled environment epsiodes, each with 100 steps.  One initial observation and a sparse reward signal at the end of each episode was provided.}
    \label{fig:finetuning}
\end{figure}

\subsection{Guided Reinforcement Learning}
\label{sec:g_ref_ler}
In this section, we test the perforance of AC given minimal expert guidance. We use the Reach and Winow Open environments with 2000 training epsiodes per run and three runs per experiment and learner. 
Per data point we sampled 50 trajectories from the test data set. 
With this setting, the algorithms must learn most of the behaviour from reinforcement learning, but the expert 
demonstration is needed to overcome the initial search problem. We only provide sparse reward and one obervsation per trajectory, thus finding the first viable solution by unguided 
trial and error is unviable for most problems. We found the "Drawer Close" environment can be learned without any expert guidance. 
For the baselines, we used RPPO, PPO and TQC with behavioural cloning as pretraining, where expert trajectories are provided. We expect TQC to be better at exploration and PPO to be more stable. The results 
are shown in figure \ref{fig:guided_ref}. 

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
      \includegraphics[width=\textwidth]{images/1_2000/Reach.png}
      \caption{1 expert demonstration.}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
      \includegraphics[width=\textwidth]{images/1_2000/Window Open.png}
      \caption{1 expert demonstration.}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
      \includegraphics[width=\textwidth]{images/0/Drawer Close.png}
      \caption{0 expert demonstrations.}
      \label{fig:drawerclose}
    \end{subfigure}
    \medskip
    \begin{subfigure}[t]{0.45\textwidth}
      \includegraphics[width=\textwidth]{images/1_2000_imi/Reach.png}
      \caption{Reach environment. Comparison between pure imitation learning and reinforcement learning.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
      \includegraphics[width=\textwidth]{images/1_2000_imi/Window Open.png}
      \caption{Window Open environment. Comparison between pure imitation learning and reinforcement learning.}
    \end{subfigure}
    \caption{
    The learner trained by PPO, RPPO and TQC were pretrained using behavioural cloning given one demonstration except in the drawer close environment. 
    The pure imitation learning experiments on the buttom row were conducted using no additional epsiodes from the environment. There, the x-axis 
    displays equal amount of compute compared to the reinforcement learning runs.
    Each experiment was repeatet three times with 
    50 sampled validation episodes per run and data point. The shaded region displays one standard deviation.}
    \label{fig:guided_ref}
\end{figure}

The AC algorithm was able to solve all environments with high success rates after 1000 or 2000 sampled trajectories. In "Reach" and "Window Open" the initial perforance 
of behavioural cloning was not improved in the reinforcement phase by any of the baselines. Similar to our discussion in section \ref{sec:fine_tuning}, 
we had to choose small learning rates to prevent catastrophic forgetting. With higher learning rates, all algorithms dropped to $0 \%$ success rate and did 
not recover in initial tests. We find from the experiments, that the challenging one observation sparse reward environment is difficult to learn in for 
traditional actor critic algorithms and that AC is a significant improvement in these setups.\\

We wanted to rule out, that the perforance difference comes mainly from our behavioural cloning setup. As PPO and TQC natively don't make use of expert demonstrations, 
we pretrained them using behavioural cloning. We expected that from the pretraining, at least one successful trajectory would be sampled in the training phase, thus 
helping to overcome the initial search problem. We find that all algorithms perform significantly above $0 \%$ success rate, so we conclude that all algorithms had 
access to guidance to correct solutions. To further rule out that our methodology gives an unequal advantage to our algorithm, we tested all algorithms on the 
"Drawer Close" environment. We found, that random trajectories can already solve the environment, so no expert guidance is needed to learn a feasable behaviour. 
The results are shown in subplot \ref{fig:drawerclose}. The different perforance at 0 environment interactions comes from different behaviour of the algorithms at 
the start of the learning phase. TQC samples actions randomly until a threshold is met, in our we set it 100 steps equal to one randomly sampled trajectory. 
PPO starts sampling actions according to the actor network from the beginning. As we repeat the initial observation $T$ times for the trajectory and only append the 
positoinal encoding, the observations tend to be close in $L_2$ distance. This might be the reason why we find that PPO produces narrowly distributed action sequences 
at the beginning of the training, which lead to poor performance. Once the robot arm is driven to an edge, it does not move anywhere else and the probability of 
closing the drawer is low. We investigated the difference between the initial performance of AC and TQC by repeating the first step of the experiment 20 times and 
found that both algorithms converge to about $65 \%$ successrate from zero environment interaction in this setup. Like PPO, RPPO does not sample randomly at the 
beginning. From the plot it can be seen that PPO steadily improves perforance until, further showcasing that it is the most stable algorithm from our baselines. 
TQC learns rapidly in some sections but seems to overshoot and fall back to bad perforance frequently. We find it to be more dependent on hyper parameters then PPO 
and RPPO. AC rapidly improved to perfect perforance within about 400 steps. Notably, we used the same set of hyper parameters for AC across all our experiments, 
which demonstrates the robustness of our approach. \\

Next we wanted to rule out, that the observed perforance gain of AC over the baselines comes mostly from the different underlying neural network architectures. AC uses transformers for the actor and 
critic, while the TQC and PPO implementations use MLPs and the RPPO implementation uses a GRU. To do this, we tested the performance of AC using pure imitation learning shown on the bottom row of 
figure \ref{fig:guided_ref}. We expect the main improvement of behavioural cloning in AC, TQC and PPO over a recurrent policy comes from the sequence encoding, as proposed in section \ref{COD_AC}.
We find that the pure imitation learning performance 
of AC is about the same as behavioural cloning for PPO and TQC. While in the Window Open environment, both PPO and TQC outperformed RPPO, we find that in the Reach environment RPPO performs 
similar to PPO. As only one expert trajectory was provided and we could only repeat the experiment twice out of time constraints, we find the difference between RPPO, PPO and TQC in the Reach 
environment too small to make conclusion about their relative perforance. \\
Overall, we take this finding as evidence for the effectiveness of the AC algorithm. It is highly unlikely that the observed improvement 
over the baselines comes mostly from the transformer architecture underlying the AC actor and critic, given the similar behavioural cloning perforance and the fact that our algorithms trains the actor and 
critic in a behavioural cloning style.  

\subsection{Comparison of Optimisation Modes}
\label{ref:com_opt_modes}
In section \ref{sec:inf_time_search} we discussed that we can use different modes of optimisation during inference time to optimize the action sequemce. In this section, 
we showcase the difference between optimizing the action sequence directly and optimizing the actor and planner weights. We tested the two different 
modes in the reach environment with the same setup as in section \ref{sec:g_ref_ler}, thus we used the same data for the actor planner optimisation runs. For the direct 
action optimisation runs we used 20000 sampled episodes to see, if reinforcement learning took place at all, as for the first 2000 epsiodes no improvement above 
imitation learning performance is seen. For this reason, we only repeated the experiment twice, as it took a long time to compute. The results are depicted in figure 
\ref{fig:action_vs_actor}. It is obvious, that the optimisation mode is key to the reinforcement learning perforance. We hypothesize a reason for the much 
faster convergence rate of the actor planner optimisation mode is, that they encode an informed representation of the space of useful trajectories with 
respect to the environment, which is guiding the search. Subfigure \ref{fig:direct_actions} shows the changes to the trajectory given the gradient from the critic is applied 
directly to the actions, while subfigure \ref{fig:ac_pl_actions} shows the changes given the gradient is applied to the actor and planner. We find 
subfigure \ref{fig:ac_pl_actions} shows a structured exploration, while subfigure \ref{fig:direct_actions} is more random. 

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[t]{0.65\textwidth}
      \includegraphics[width=\textwidth]{images/Plan_vs_Actions/Reach.png}
      \caption{Reach environment. Shown is the difference in learning efficiency of the Active Critic algorithm using two different optimisation modes.}
      \label{fig:plot1}
    \end{subfigure}
    \medskip
    \begin{subfigure}[t]{0.45\textwidth}
      \includegraphics[width=\textwidth]{images/Plan_vs_Actions/changes/actions_1.png}
      \caption{Optimized trajectories (orange) and trajectories proposed by the actor (green). The gradient from the critic was applied to the actions.}
      \label{fig:direct_actions}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
      \includegraphics[width=\textwidth]{images/Plan_vs_Actions/changes/plans_actor_0.png}
      \caption{Optimized trajectories (orange) and trajectories proposed by the actor (green). The gradient from the critic was applied to the actor and planner.}
      \label{fig:ac_pl_actions}
    \end{subfigure}
    \caption{Comparison of two different optimisation modi. Each experiment was repeated twice with 30 evaluation episodes per data point.
    The x-axis shows the number of sampled environment epsiodes, each with 100 steps. One initial observation and a sparse reward signal at the end of each episode was provided.}
    \label{fig:action_vs_actor}
\end{figure}

\subsection{Dense Observations}
In this section we test the relaxation of the AC algorithm to MDP environments with the environment state given as an observation after each step, as developed in section \ref{sec:relax_dense}. 
We use sparse rewards 
and compare "Dense Observation Active Critic" (DOAC) to PPO and TQC on the "Reach", "Window Open" and "Drawer Close" environments. Apart from 
providing an observation per time step,
we use the same setup as discribed in previous sections. The results are shown in figure \ref{fig:dense_ref}. First, we find that both base line algorithms 
start at similar perforance from behavioural cloning as in the single observation setup, which is expected, since behavioural cloning does not rely on 
environment interactions. After the first data point, both algorithms sharply drop to near zero perforance, which is worse perforance then for the single 
observation runs. We explain this with the distribution shift after training starts. As discussed earlier, a policy trained by imitation learning 
will induce a different distribution then the expert data. This breaks the i.i.d. assumption. In the single observation case for PPO and TQC, the i.i.d. assumption was not broken, 
as the inputs are only the first observtion which distribution is a constant of the environment. In the MDP setup with dense observations, most input data is 
sampled according to the current policy and thus the i.i.d. assumption is broken. PPO did not find solutions to the "Reach" and "Window Open" environments, 
but we find TQC got about $20 \%$ success rate on "Reach" and achieved about $80 \%$ success rate on "Window Open" leveling out after about 2500 episodes. 
The superior perforance of TQC over PPO is expected, as TQC tends to perfrom better in environments where exploration is important. As TQC now gets 
feedback from it's actions on the environment, the better exploration is an advantage over PPO.\\ 
TQCs perforance drops to about $0 \%$ for the "Window Open" 
environment after the first data point before environment interaction. We wanted to analyze, if TQC uses the guidance from the expert at all, or if it starts from 
no usable knowledge about the environment. To do this, we also conducted the experiment three times with no initial behavioural cloning. The results are shown in 
subfigure \ref{fig:TQC_0_vs_exp}. We find that TQC can solve the environment within 5000 steps from no expert demonstrations in one case, but does not find a good policy in the other two. 
We propose that the pretraining from behavioural cloning guides is a useful bias for the algorithm to find feasable solutions quicker with higher probability. Following this insight, 
we also conducted the "Window Open" experiment in MDP setting with GAIL to make better use of the provided expert demonstration. 

###Result from GAIL

However, DOAC is the oly algorithm that can greatly benifit from the expert demonstration and 
it outperforms all baselines easily. \\

We also included results for the "Drawer Close" environment with no expert demonstrations. All algorithms perform better then in the single observation environment, 
but DOAC is the only algorithm with stable perforance, meaning it found a solution with near perfect success rate in all three runs quickly. \\
Overall, DOAC performs well in all shown environments. Especially in the "Reach" environment it is the only algorithm that finds a feasable solution while it 
converges much quicker and has a more stable bahaviour in general. 

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
    \includegraphics[width=\textwidth]{images/dense_1/Reach.png}
    \caption{One expert demonstration.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.45\textwidth}
    \includegraphics[width=\textwidth]{images/dense_0/Drawer Close.png}
    \caption{No expert demonstrations.}
  \end{subfigure}
  \medskip
  \begin{subfigure}[t]{0.45\textwidth}
    \includegraphics[width=\textwidth]{images/dense_1/Window Open.png}
    \caption{One expert demonstration.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.45\textwidth}
    \includegraphics[width=\textwidth]{images/TQC_bc_GAIL_vs_ref/Window Open.png}
    \caption{Comparison between TQC with no expert demonstrations and expert guided baselines.}
    \label{fig:TQC_0_vs_exp}
  \end{subfigure}
  \caption{Comparison between DOAC and various baselines given one or no expert demonstrations. The environment state was provided after each step.
  Each experiment was repeated three times with 50 evaluation episodes per data point.}
  \label{fig:dense_ref}
\end{figure}

\subsection{Comparison of Dense and Single Observation}
In this section we compare DOAC and AC, to investigate if DOAC makes use of the additionally provided information. The results are shown in figure \ref{fig:dense_vs_single}.
We find that in "Reach" and "Drawer Close", DOAC converges quicker. Especially in "Reach", we see a stable improvement from DOAC over AC. In "Drawer Close", the difference is 
small, which is mainly due to the fact that AC already solves the environment very quickly. Note hoever, that DOAC had worse initial perforance then AC in the shown experiments. 
As we discussed earlier, the initial perforance comes from the random initialisation and varies greatly. We take the fact that DOAC converged quicker then AC even though it had 
worse starting initialisations in the three runs as further evidence, that DOAC makes effective use of the additional information. In the "Window Open" environment, DOAC initially converged slower but 
reached near pefect performance quicker then AC. We suppose this is due to the different optimisation modes used for the two algorithms. Recall that AC optimizes the whole 
trajectory at the beginning, while DOAC makes small improvements per time step, due to self imposed inference time constraints. We expect this has the effect that in inference time, the critic of 
AC can make larger changes to the trajectory proposed by the actor. In other words, while the actor converges DOAC makes smaller moves towards the critic target then AC. Also out of time constraints, we could 
not conduct a hyper parameter search for DOAC. It is possible that DOAC would converge quicker, given more optimisation steps per inference step or a higher inference optimisation 
learning rate $\alpha_{inf}$.\\ 
AC already performs close to the MDP setup used by DOAC. We propose this is due to the fact, that AC uses the information about the environment 
from the expert demonstrations effectively to search for new candidate solutions. As discussed in section \ref{ref:com_opt_modes}, AC searches on a preimposed subspace given the "knowledge" of the 
actor. This means it does not conduct random search even though it does not not get any feedback from the environment. This is probably a key for the good perforance of AC and DOAC and explains 
the relative close perforance of both algorithms. We expect a major advantage of DOAC over AC in non deterministic environments, as AC sould not be able to solve those from the initial 
observation alone. DOAC already has a method to disambiguate action sequences given the same observations as discussed in section \ref{avr_action_problem}, which is key, if the same initial 
observations and actions can lead to different observations at later time steps. However out of time constraints, we leave this research question to future work.\\
Overall we find that DOAC works well in the tested environments and outperforms all baselines in the MDP setting with sparse rewads.
\begin{figure}[htbp]
  \centering
  \begin{subfigure}[t]{0.32\textwidth}
    \includegraphics[width=\textwidth]{images/dense_vs_sparse_1/Reach.png}
    \caption{1 expert demonstration.}
  \end{subfigure}
  \medskip
  \begin{subfigure}[t]{0.32\textwidth}
    \includegraphics[width=\textwidth]{images/dense_vs_sparse_1/Window Open.png}
    \caption{1 expert demonstration.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.32\textwidth}
    \includegraphics[width=\textwidth]{images/dense_vs_sparse_0/Window Open.png}
    \caption{0 expert demonstrations.}
  \end{subfigure}
  \caption{Comparison of AC in single observation and DOAC in mdp environments. Each experiment was repeated three times with 50 evaluation episodes per data point.
  }
  \label{fig:dense_vs_single}
\end{figure}